<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>ML Crashcourse Index</title>
  <meta name="description" content="ML Crashcourse Index">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="ML Crashcourse Index" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="<a href="https://github.com/uodatascience/ML-CrashCourse" class="uri">https://github.com/uodatascience/ML-CrashCourse</a>" />
  
  
  <meta name="github-repo" content="uodatascience/ML-CrashCourse" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="ML Crashcourse Index" />
  
  
  

<meta name="author" content="UO Data Science">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="regularized-regression-in-caret.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Index</a></li>
<li class="chapter" data-level="2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>2</b> What is Machine Learning?</a><ul>
<li class="chapter" data-level="2.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#a-broad-definition"><i class="fa fa-check"></i><b>2.1</b> A Broad Definition…</a></li>
<li class="chapter" data-level="2.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#major-classes-of-ml-problems"><i class="fa fa-check"></i><b>2.2</b> Major Classes of ML Problems</a></li>
<li class="chapter" data-level="2.3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#a-brief-history"><i class="fa fa-check"></i><b>2.3</b> A Brief History…</a></li>
<li class="chapter" data-level="2.4" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#common-ml-algorithms"><i class="fa fa-check"></i><b>2.4</b> Common ML Algorithms</a><ul>
<li class="chapter" data-level="2.4.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#decision-trees"><i class="fa fa-check"></i><b>2.4.1</b> <strong>Decision Trees</strong></a></li>
<li class="chapter" data-level="2.4.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#support-vector-machines"><i class="fa fa-check"></i><b>2.4.2</b> <strong>Support Vector Machines</strong></a></li>
<li class="chapter" data-level="2.4.3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#artificial-neural-networks"><i class="fa fa-check"></i><b>2.4.3</b> <strong>Artificial Neural Networks</strong></a></li>
<li class="chapter" data-level="2.4.4" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>2.4.4</b> <strong>k-Means Clustering</strong></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#resources"><i class="fa fa-check"></i><b>2.5</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html"><i class="fa fa-check"></i><b>3</b> Regularized Regression in Caret</a><ul>
<li class="chapter" data-level="3.1" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#the-bias-variance-tradeoff-overfitting-and-cross-validation"><i class="fa fa-check"></i><b>3.1</b> The Bias-Variance Tradeoff, Overfitting, and Cross-Validation</a><ul>
<li class="chapter" data-level="3.1.1" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>3.1.1</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="3.1.2" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#overfitting"><i class="fa fa-check"></i><b>3.1.2</b> Overfitting</a></li>
<li class="chapter" data-level="3.1.3" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#regularization"><i class="fa fa-check"></i><b>3.2</b> Regularization</a><ul>
<li class="chapter" data-level="3.2.1" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#ridge-all-of-these-features-matter-but-only-a-little-bit."><i class="fa fa-check"></i><b>3.2.1</b> Ridge: all of these features matter, but only a little bit.</a></li>
<li class="chapter" data-level="3.2.2" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#lasso-only-some-features-matter-and-they-might-matter-a-lot"><i class="fa fa-check"></i><b>3.2.2</b> Lasso: only some features matter, and they might matter a lot</a></li>
<li class="chapter" data-level="3.2.3" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#elastic-net-maybe-everything-matters-and-maybe-only-a-little-bit."><i class="fa fa-check"></i><b>3.2.3</b> Elastic Net: maybe everything matters, and maybe only a little bit.</a></li>
<li class="chapter" data-level="3.2.4" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#example-using-caret"><i class="fa fa-check"></i><b>3.2.4</b> Example using Caret</a></li>
<li class="chapter" data-level="3.2.5" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#extracting-features-a-brief-detour-into-text-analysis"><i class="fa fa-check"></i><b>3.2.5</b> Extracting features: a brief detour into text analysis</a></li>
<li class="chapter" data-level="3.2.6" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#modeling-with-caret"><i class="fa fa-check"></i><b>3.2.6</b> Modeling with caret</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#closing-thoughts"><i class="fa fa-check"></i><b>3.3</b> Closing thoughts</a></li>
<li class="chapter" data-level="3.4" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#references"><i class="fa fa-check"></i><b>3.4</b> References:</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ML Crashcourse Index</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="what-is-machine-learning" class="section level1">
<h1><span class="header-section-number">2</span> What is Machine Learning?</h1>
<div id="a-broad-definition" class="section level2">
<h2><span class="header-section-number">2.1</span> A Broad Definition…</h2>
<p>Machine learning is an approach to making accurate predictions using experience (ie. prior information). The methods that arise from this approach attempt to algorithmically fit some function relating some “input” values to some “output” values such that the function accurately predicts future outputs from future inputs.</p>
<p>Or, the goal is to determine the relationship between thing a (or things a1, …, an) and thing b (or things b1, …, bn) even when that relationship is arbitrarily complex.</p>
<p>Once more, according to Vapnik “the learning process is a process of choosing an appropriate function from a given set of functions.”</p>
<p>For example, linear regression works great when the relationship is linear, but poorly when it looks like this…</p>
<div class="figure">
<img src="files/fox_rabbit.png" />

</div>
<p>If the only class of functions you were considering were lines of different slopes, you would fail to characterize the relationship between the three variables (time, fox, and rabbit populations). Machine learning is a generalization of the linear/parametric statistical problems you are likely familiar with.</p>
<p>Our learning machines have two basic requirements:</p>
<ol style="list-style-type: decimal">
<li>To estimate our function from a wide variety of functions – roughly, we want to be able to estimate the relationship no matter how complicated it might be.</li>
<li>To estimate our function using a limited number of examples.</li>
</ol>
</div>
<div id="major-classes-of-ml-problems" class="section level2">
<h2><span class="header-section-number">2.2</span> Major Classes of ML Problems</h2>
<ul>
<li><strong>Classification/Pattern Recognition</strong></li>
</ul>
<p>Given a set of observations, learn a generalizable rule that allows their discrete class to be predicted. For example, we see a series of animals with either two or four legs who do or don’t bark. We want to learn to categorize animals with two legs as humans, animals with four legs who don’t bark as cats and animals with four legs who do bark as dogs. We want to <em>partition</em> the input space.</p>
<div class="figure">
<img src="files/classification.png" />

</div>
<ul>
<li><strong>Regression</strong></li>
</ul>
<p>Given a set of observations, learn a generalizable relationship that allows some real-valued output to be predicted. For example, we want to estimate a likely temperature after seeing a series of temperatures, times, humidities, air pressures, etc.</p>
<div class="figure">
<img src="files/regression.png" />

</div>
<ul>
<li><strong>Density Estimation</strong></li>
</ul>
<p>Given a set of observations, we want to describe the probability distribution of the underlying population. The most commonly associated machine learning methods are those of cluster analysis, which seek to characterize the structure of data without knowing about category membership in advance – things that share a probability distribution are more likely to be from the same phenomenon than those that do not.</p>
</div>
<div id="a-brief-history" class="section level2">
<h2><span class="header-section-number">2.3</span> A Brief History…</h2>
<p>(taken largely from Ch. 1 of Vapnik’s Statistical Learning Theory)</p>
<ul>
<li><p><strong>The parametric approach of the 1920-30’s</strong> - The classical, maximum likelihood methods developed largely by Fisher in the 1920’s pose the problem of statistical inference as 1) assuming the structure of the process that generated observed values – eg. the assumption of normality – in order to 2) estimate the parameters of a predefined function. This parametric method of inference works well when 1) the laws that generate the random error of observation and 2) the form of the function whose parameters are to be estimated are both known in advance. The number of problems that satisfy those two conditions are very few indeed. This approach reached its “golden age” from 1930-1960, but remains in common currency in some part by historical accident (its alternative was developed by the Russians during the Cold War) and in some part by many scientists being unwilling to give half an ass to learn math.</p></li>
<li><p><strong>Empirical Risk Minimization in the 1960’s</strong> - It is easy to forget that many of the classical methods were developed without computers, and evaluating complex datasets was laborious such that it neared impossibility. When we got our hands on our computers it became clear that the ideas that underlie the parametric approach to inference were naive, however convenient. It is from this realization that the practice of “data analysis” that seeks to describe data rather than make formal statistical inductions followed. The development of the <a href="https://en.wikipedia.org/wiki/Perceptron">perceptron</a>, which mimics very crude neurons that update input “weights” depending on repeated input to estimate category labels prompted the development of <em>Emperical Risk Minimization</em>. The ERM principle suggests that functions that generalize well to future inputs, ie. functions that describe the input/output relationship well, should be trained by minimizing the observed error (risk) on training examples. The major intellectual developments that followed were the description of the conditions for <em>consistency</em> of ERM – when ERM will arrive at the best possible solution – and the <em>quality</em> of the functions fit by ERM.</p></li>
<li><p><strong>Backpropagation in the 1980’s</strong> - This practice remained unpopular due to the limitations of perceptions and related learning algorithms, but the development of backpropagation that allowed multiple instances of a learning algorithm to be chained together renewed interest. Backpropagation allows the observed error at the output of a chain of stacked algorithms to be… propagated… backwards… through to the input, so that all weights or parameters of the function estimator can be intelligibly updated rather than just those of the output.</p></li>
<li><p><strong>Deep Learning in the 2000’s</strong> - Backpropagation is great, but computationally expensive. The advent of cheap GPUs that allow the matrix algebra necessary for deep algorithms to be performed in a tractable period of time.</p></li>
</ul>
<p>From this history we can see the major components of the machine learning approach:</p>
<ol style="list-style-type: decimal">
<li><p>Models are evaluated by a loss value: the difference between the correct answer and the predicted answer</p></li>
<li><p>Models are improved by updating their parameters to minimize that loss value</p></li>
<li><p>Multiple models, or units of models, are combined so increase the generality of the functions that can be estimated.</p></li>
<li><p>Models are tested by their ability to generalize to novel data, or by ‘cross-validation’</p></li>
</ol>
</div>
<div id="common-ml-algorithms" class="section level2">
<h2><span class="header-section-number">2.4</span> Common ML Algorithms</h2>
<p>Good god there are so many. The folks that put together scikit-learn for Python made this flowchart that gives a very high-level overview of the major types</p>
<div class="figure">
<img src="files/ml_map.png" />

</div>
<div id="decision-trees" class="section level3">
<h3><span class="header-section-number">2.4.1</span> <strong>Decision Trees</strong></h3>
<p>For this I defer to this lovely presentation: <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/" class="uri">http://www.r2d3.us/visual-intro-to-machine-learning-part-1/</a></p>
</div>
<div id="support-vector-machines" class="section level3">
<h3><span class="header-section-number">2.4.2</span> <strong>Support Vector Machines</strong></h3>
<p>In the simplest example, we learn some line, plane, or hyperplane that best separates classes.</p>
<div class="figure">
<img src="files/svn_planes.png" alt="H1 does not separate the classes. H2 does, but only with a small margin. H3 separates them with the maximum margin." />
<p class="caption">H1 does not separate the classes. H2 does, but only with a small margin. H3 separates them with the maximum margin.</p>
</div>
</div>
<div id="artificial-neural-networks" class="section level3">
<h3><span class="header-section-number">2.4.3</span> <strong>Artificial Neural Networks</strong></h3>
<p>Models are composed of neurons, arranged in layers, with particular connectivity between (or within) layers. Neurons can be arbitrarily complex, from the simplest weight-only neuron to the hypercomplex LSTM neurons. Models also get their power from their architecture, as exemplified convolutional neural networks.</p>
<div class="figure">
<img src="files/an.jpg" alt="A neuron. Outputs are computed by multiplying inputs by trained weights, summing them, and passing that sum to some typically nonlinear function. http://www.theprojectspot.com/tutorial-post/introduction-to-artificial-neural-networks-part-1/7" />
<p class="caption">A neuron. Outputs are computed by multiplying inputs by trained weights, summing them, and passing that sum to some typically nonlinear function. <a href="http://www.theprojectspot.com/tutorial-post/introduction-to-artificial-neural-networks-part-1/7" class="uri">http://www.theprojectspot.com/tutorial-post/introduction-to-artificial-neural-networks-part-1/7</a></p>
</div>
<div class="figure">
<img src="files/ANN-Diagram.png" alt="A simple multilayer, fully-connected ANN" />
<p class="caption">A simple multilayer, fully-connected ANN</p>
</div>
<div class="figure">
<img src="files/Typical_cnn.png" alt="Convolutional Neural Nets learn simplified features by restricting their connectivity" />
<p class="caption">Convolutional Neural Nets learn simplified features by restricting their connectivity</p>
</div>
<p>And of course we have to <a href="https://www.youtube.com/watch?v=SCE-QeDfXtA">deep dream a little bit</a></p>
</div>
<div id="k-means-clustering" class="section level3">
<h3><span class="header-section-number">2.4.4</span> <strong>k-Means Clustering</strong></h3>
<p>Divide data into k clusters such that each observation is identified as the cluster with the nearest mean value. One of the simplest clustering algorithms.</p>
<div class="figure">
<img src="files/K-means_convergence.gif" />

</div>
</div>
</div>
<div id="resources" class="section level2">
<h2><span class="header-section-number">2.5</span> Resources</h2>
<ul>
<li><p><a href="https://mitpress.mit.edu/books/foundations-machine-learning">Foundations of Machine Learning</a></p></li>
<li><p><a href="http://math.arizona.edu/~hzhang/math574m/Read/vapnik.pdf">Vapnik 1999 - An Overview of Statistical Learning Theory</a></p></li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-1-4757-2440-0_1">Vapnik - The Nature of Statistical Learning Theory</a></p></li>
<li><p><a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/">A tour of ML algorithms</a></p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regularized-regression-in-caret.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
