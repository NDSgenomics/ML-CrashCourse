<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>ML Crashcourse Index</title>
  <meta name="description" content="ML Crashcourse Index">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="ML Crashcourse Index" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="<a href="https://github.com/uodatascience/ML-CrashCourse" class="uri">https://github.com/uodatascience/ML-CrashCourse</a>" />
  
  
  <meta name="github-repo" content="uodatascience/ML-CrashCourse" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="ML Crashcourse Index" />
  
  
  

<meta name="author" content="UO Data Science">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="what-is-machine-learning.html">
<link rel="next" href="regularization.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Index</a></li>
<li class="chapter" data-level="2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>2</b> What is Machine Learning?</a><ul>
<li class="chapter" data-level="2.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#a-broad-definition"><i class="fa fa-check"></i><b>2.1</b> A Broad Definition…</a></li>
<li class="chapter" data-level="2.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#major-classes-of-ml-problems"><i class="fa fa-check"></i><b>2.2</b> Major Classes of ML Problems</a></li>
<li class="chapter" data-level="2.3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#a-brief-history"><i class="fa fa-check"></i><b>2.3</b> A Brief History…</a></li>
<li class="chapter" data-level="2.4" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#common-ml-algorithms"><i class="fa fa-check"></i><b>2.4</b> Common ML Algorithms</a><ul>
<li class="chapter" data-level="2.4.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#decision-trees"><i class="fa fa-check"></i><b>2.4.1</b> <strong>Decision Trees</strong></a></li>
<li class="chapter" data-level="2.4.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#support-vector-machines"><i class="fa fa-check"></i><b>2.4.2</b> <strong>Support Vector Machines</strong></a></li>
<li class="chapter" data-level="2.4.3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#artificial-neural-networks"><i class="fa fa-check"></i><b>2.4.3</b> <strong>Artificial Neural Networks</strong></a></li>
<li class="chapter" data-level="2.4.4" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>2.4.4</b> <strong>k-Means Clustering</strong></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#resources"><i class="fa fa-check"></i><b>2.5</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-bias-variance-tradeoff-overfitting-and-cross-validation.html"><a href="the-bias-variance-tradeoff-overfitting-and-cross-validation.html"><i class="fa fa-check"></i><b>3</b> The Bias-Variance Tradeoff, Overfitting, and Cross-Validation</a><ul>
<li class="chapter" data-level="3.1" data-path="the-bias-variance-tradeoff-overfitting-and-cross-validation.html"><a href="the-bias-variance-tradeoff-overfitting-and-cross-validation.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>3.1</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="3.2" data-path="the-bias-variance-tradeoff-overfitting-and-cross-validation.html"><a href="the-bias-variance-tradeoff-overfitting-and-cross-validation.html#overfitting"><i class="fa fa-check"></i><b>3.2</b> Overfitting</a></li>
<li class="chapter" data-level="3.3" data-path="the-bias-variance-tradeoff-overfitting-and-cross-validation.html"><a href="the-bias-variance-tradeoff-overfitting-and-cross-validation.html#cross-validation"><i class="fa fa-check"></i><b>3.3</b> Cross-Validation</a><ul>
<li class="chapter" data-level="3.3.1" data-path="the-bias-variance-tradeoff-overfitting-and-cross-validation.html"><a href="the-bias-variance-tradeoff-overfitting-and-cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>3.3.1</b> K-fold cross-validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>4</b> Regularization</a><ul>
<li class="chapter" data-level="4.1" data-path="regularization.html"><a href="regularization.html#ridge-all-of-these-features-matter-but-only-a-little-bit."><i class="fa fa-check"></i><b>4.1</b> Ridge: all of these features matter, but only a little bit.</a></li>
<li class="chapter" data-level="4.2" data-path="regularization.html"><a href="regularization.html#lasso-only-some-features-matter-and-they-might-matter-a-lot"><i class="fa fa-check"></i><b>4.2</b> Lasso: only some features matter, and they might matter a lot</a></li>
<li class="chapter" data-level="4.3" data-path="regularization.html"><a href="regularization.html#elastic-net-maybe-everything-matters-and-maybe-only-a-little-bit."><i class="fa fa-check"></i><b>4.3</b> Elastic Net: maybe everything matters, and maybe only a little bit.</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ML Crashcourse Index</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-bias-variance-tradeoff-overfitting-and-cross-validation" class="section level1">
<h1><span class="header-section-number">3</span> The Bias-Variance Tradeoff, Overfitting, and Cross-Validation</h1>
<div id="bias-variance-tradeoff" class="section level2">
<h2><span class="header-section-number">3.1</span> Bias-Variance Tradeoff</h2>
<p>This came up briefly last week, but the bias-variance tradeoff is really crucial to everything we’ll talk about today. The basic idea is that you can partition error into two components: bias and variance. <strong>Bias</strong> refers to the extent to which a model produces parameter estimates that miss in a particular direction (e.g., consistently over-estimating or consistently under-estimating a parameter value). <strong>Variance</strong> refers to the extent to which a model produces parameter estimates that vary from their central tendency across different datasets.</p>
<p>There is a tradeoff between bias and variance: all else equal, increasing bias will decrease variance. The basic idea here is that if we have a zero-bias estimator, it will tend to try to fit everything in the data, whereas an estimator biased in some direction won’t. This isn’t to say that bias is good or bad, just that there are times where we might want to increase bias to decrease variance. As we’ll see later, regularization is basically one method for introducing bias (to minimize variance).</p>
</div>
<div id="overfitting" class="section level2">
<h2><span class="header-section-number">3.2</span> Overfitting</h2>
<p>This is probably familiar to folks in here (and it came up last week), so I won’t say much about it here. The basic idea is that any data has signal and noise. Sometimes, something appears to be signal but is actually noise. That is, when our statistical models are searching for the best solution, they sometimes will be fooled into thinking some noise is signal. This is usually called <strong>overfitting</strong>, and it has presented a pretty substantial problem in statistical modeling. As you’ll see, one of our goals is to try to avoid overfitting. Also worth noting is that overfitting will tend to produce a model with high variance, because noise will vary from dataset to dataset (basically by definition), and so a model which has fit noise will not do well across different datasets (with different noise).</p>
</div>
<div id="cross-validation" class="section level2">
<h2><span class="header-section-number">3.3</span> Cross-Validation</h2>
<p>Cross-validation generally refers to taking a model that you trained on some data and using it in a new dataset. Unlike a replication, the model parameters carry over from the training to the test data (i.e., you don’t simply use the same variables and re-estimate the model parameters; you save the model parameters, and use it to predict the outcome variable). You can use cross-validation both to train and evaluate a model. A simple example may make this clear.</p>
<p>Let’s say we think home size (in square-feet) is the only relevant predictor for house price. So, we have some data on prices of recently sold houses, and estimate a model predicting house price from square-feet:</p>
<p><span class="math display">\[y_{price} = b_0 + b_1*sqaurefeet\]</span></p>
<p>Let’s say we get these parameter values:</p>
<p><span class="math display">\[y_{price} = 100 + 50*sqaurefeet\]</span></p>
<p>And now we want to cross-validate in a hold-out sample. We wouldn’t simply estimate this model again:</p>
<p><span class="math display">\[y_{price} = b_0 + b_1*sqaurefeet\]</span></p>
<p>We would instead apply this model:</p>
<p><span class="math display">\[y_{price} = 100 + 50*sqaurefeet\]</span> And evaluate how well it did. We could do this either by how much it misses, which is usually done with root mean squared error (RMSE):</p>
<p><span class="math display">\[MSE = \frac{1}{n}\sum\limits_{i=1}^{n}(y_i - \hat{y_i})^2\]</span> <span class="math display">\[RMSE = \sqrt{MSE}\]</span></p>
<p>Typically, people will also look at prediction accuracy, using the model’s <span class="math inline">\(R^2\)</span>. This is interpreted the same way as <span class="math inline">\(R^2\)</span> always is (as the % of variance in the outcome accounted for by the model).</p>
<div id="k-fold-cross-validation" class="section level3">
<h3><span class="header-section-number">3.3.1</span> K-fold cross-validation</h3>
<p>There are different varieties of cross-validation. The most intuitive version is to create a single partition of data (i.e., split full data frame into two dataframes: training and test). However, there are other methods for cross-validation. One that has been gaining steam (or is maybe already at full steam at this point) is <strong>k-fold cross-validation</strong>. The basic idea is that we split a dataset into k subsamples (called folds). We then treat one subsample as the holdout sample, train on the remaining subsamples, and cross-validate on the holdout sample; then rinse and repeat so to speak. An example will probably help here.</p>
<p>Let’s take a ridiculously simple example (based on the earlier example). We want to predict house sale price from square footage:</p>
<p><span class="math display">\[y_{price} = b_0 + b_1*sqaurefeet\]</span></p>
<p>Let’s say we have just 30 cases, and we use 10-fold cross validation. Let each observation be indicated by <span class="math inline">\(o_i\)</span>, so the first observation is <span class="math inline">\(o_1\)</span>, the second is <span class="math inline">\(o_2\)</span>, and the third is <span class="math inline">\(o_3\)</span>, etc.</p>
<p>First, we would fit a model using folds 2 through 10 (i.e., <span class="math inline">\(o_4\)</span> to <span class="math inline">\(o_30\)</span>), and then test it on the first fold (<span class="math inline">\(o_1\)</span> to <span class="math inline">\(o_3\)</span>). Then, we would fit the model using folds 1 and 3-10 (i.e., <span class="math inline">\(o_1\)</span> to <span class="math inline">\(o_3\)</span> &amp; <span class="math inline">\(o_7\)</span> to <span class="math inline">\(o_30\)</span>) and test it on the 2nd fold (<span class="math inline">\(o_4\)</span>, <span class="math inline">\(o_5\)</span>, <span class="math inline">\(o_6\)</span>), and so on until each fold was used as the holdout sample.</p>
<p>Then, we calculate the average performance across all of the tests.</p>
<p>Note that you can also use k-fold cross-validation for training purposes. Basically, this works by taking the best fitting model from a k-fold cross-validation procedure, and then testing it on a new holdout sample.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="what-is-machine-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regularization.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
