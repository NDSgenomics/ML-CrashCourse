<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>ML Crashcourse Index</title>
  <meta name="description" content="ML Crashcourse Index">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="ML Crashcourse Index" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="<a href="https://github.com/uodatascience/ML-CrashCourse" class="uri">https://github.com/uodatascience/ML-CrashCourse</a>" />
  
  
  <meta name="github-repo" content="uodatascience/ML-CrashCourse" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="ML Crashcourse Index" />
  
  
  

<meta name="author" content="UO Data Science">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceLine, a.sourceLine { display: inline-block; min-height: 1.25em; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; }
@media print {
code.sourceCode { white-space: pre-wrap; }
div.sourceLine, a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource div.sourceLine, .numberSource a.sourceLine
  { position: relative; }
pre.numberSource div.sourceLine::before, .numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em; }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; color: #aaaaaa;  padding-left: 4px; }
@media screen {
a.sourceLine::before { text-decoration: underline; color: initial; }
}
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.bn { color: #40a070; } /* BaseN */
code span.fl { color: #40a070; } /* Float */
code span.ch { color: #4070a0; } /* Char */
code span.st { color: #4070a0; } /* String */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.ot { color: #007020; } /* Other */
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.fu { color: #06287e; } /* Function */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code span.cn { color: #880000; } /* Constant */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.ss { color: #bb6688; } /* SpecialString */
code span.im { } /* Import */
code span.va { color: #19177c; } /* Variable */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.op { color: #666666; } /* Operator */
code span.bu { } /* BuiltIn */
code span.ex { } /* Extension */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.at { color: #7d9029; } /* Attribute */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path=""><a href="#index"><i class="fa fa-check"></i><b>1</b> Index</a></li>
<li class="chapter" data-level="2" data-path=""><a href="#what-is-machine-learning"><i class="fa fa-check"></i><b>2</b> What is Machine Learning?</a><ul>
<li class="chapter" data-level="2.1" data-path=""><a href="#a-broad-definition"><i class="fa fa-check"></i><b>2.1</b> A Broad Definition…</a></li>
<li class="chapter" data-level="2.2" data-path=""><a href="#major-classes-of-ml-problems"><i class="fa fa-check"></i><b>2.2</b> Major Classes of ML Problems</a></li>
<li class="chapter" data-level="2.3" data-path=""><a href="#a-brief-history"><i class="fa fa-check"></i><b>2.3</b> A Brief History…</a></li>
<li class="chapter" data-level="2.4" data-path=""><a href="#common-ml-algorithms"><i class="fa fa-check"></i><b>2.4</b> Common ML Algorithms</a><ul>
<li class="chapter" data-level="2.4.1" data-path=""><a href="#decision-trees"><i class="fa fa-check"></i><b>2.4.1</b> <strong>Decision Trees</strong></a></li>
<li class="chapter" data-level="2.4.2" data-path=""><a href="#support-vector-machines"><i class="fa fa-check"></i><b>2.4.2</b> <strong>Support Vector Machines</strong></a></li>
<li class="chapter" data-level="2.4.3" data-path=""><a href="#artificial-neural-networks"><i class="fa fa-check"></i><b>2.4.3</b> <strong>Artificial Neural Networks</strong></a></li>
<li class="chapter" data-level="2.4.4" data-path=""><a href="#k-means-clustering"><i class="fa fa-check"></i><b>2.4.4</b> <strong>k-Means Clustering</strong></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path=""><a href="#resources"><i class="fa fa-check"></i><b>2.5</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path=""><a href="#regularized-regression-in-caret"><i class="fa fa-check"></i><b>3</b> Regularized Regression in Caret</a><ul>
<li class="chapter" data-level="3.1" data-path=""><a href="#the-bias-variance-tradeoff-overfitting-and-cross-validation"><i class="fa fa-check"></i><b>3.1</b> The Bias-Variance Tradeoff, Overfitting, and Cross-Validation</a><ul>
<li class="chapter" data-level="3.1.1" data-path=""><a href="#bias-variance-tradeoff"><i class="fa fa-check"></i><b>3.1.1</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="3.1.2" data-path=""><a href="#overfitting"><i class="fa fa-check"></i><b>3.1.2</b> Overfitting</a></li>
<li class="chapter" data-level="3.1.3" data-path=""><a href="#cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path=""><a href="#regularization"><i class="fa fa-check"></i><b>3.2</b> Regularization</a></li>
<li class="chapter" data-level="3.3" data-path=""><a href="#ridge-all-of-these-features-matter-but-only-a-little-bit."><i class="fa fa-check"></i><b>3.3</b> Ridge: all of these features matter, but only a little bit.</a></li>
<li class="chapter" data-level="3.4" data-path=""><a href="#lasso-only-some-features-matter-and-they-might-matter-a-lot"><i class="fa fa-check"></i><b>3.4</b> Lasso: only some features matter, and they might matter a lot</a></li>
<li class="chapter" data-level="3.5" data-path=""><a href="#elastic-net-maybe-everything-matters-and-maybe-only-a-little-bit."><i class="fa fa-check"></i><b>3.5</b> Elastic Net: maybe everything matters, and maybe only a little bit.</a></li>
<li class="chapter" data-level="3.6" data-path=""><a href="#example-using-caret"><i class="fa fa-check"></i><b>3.6</b> Example using Caret</a><ul>
<li class="chapter" data-level="3.6.1" data-path=""><a href="#extracting-features-a-brief-detour-into-text-analysis"><i class="fa fa-check"></i><b>3.6.1</b> Extracting features: a brief detour into text analysis</a></li>
<li class="chapter" data-level="3.6.2" data-path=""><a href="#modeling-with-caret"><i class="fa fa-check"></i><b>3.6.2</b> Modeling with caret</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path=""><a href="#closing-thoughts"><i class="fa fa-check"></i><b>4</b> Closing thoughts</a></li>
<li class="chapter" data-level="5" data-path=""><a href="#references"><i class="fa fa-check"></i><b>5</b> References:</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ML Crashcourse Index</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">ML Crashcourse Index</h1>
<h4 class="author"><em>UO Data Science</em></h4>
<h4 class="date"><em>Winter 2018</em></h4>
</div>
<section id="index" class="level1">
<h1><span class="header-section-number">1</span> Index</h1>
<ol type="1">
<li>What Is Machine Learning?</li>
</ol>

</section>
<section id="what-is-machine-learning" class="level1">
<h1><span class="header-section-number">2</span> What is Machine Learning?</h1>
<section id="a-broad-definition" class="level2">
<h2><span class="header-section-number">2.1</span> A Broad Definition…</h2>
<p>Machine learning is an approach to making accurate predictions using experience (ie. prior information). The methods that arise from this approach attempt to algorithmically fit some function relating some “input” values to some “output” values such that the function accurately predicts future outputs from future inputs.</p>
<p>Or, the goal is to determine the relationship between thing a (or things a1, …, an) and thing b (or things b1, …, bn) even when that relationship is arbitrarily complex.</p>
<p>Once more, according to Vapnik “the learning process is a process of choosing an appropriate function from a given set of functions.”</p>
<p>For example, linear regression works great when the relationship is linear, but poorly when it looks like this…</p>
<p><img src="files/fox_rabbit.png" /></p>
<p>If the only class of functions you were considering were lines of different slopes, you would fail to characterize the relationship between the three variables (time, fox, and rabbit populations). Machine learning is a generalization of the linear/parametric statistical problems you are likely familiar with.</p>
<p>Our learning machines have two basic requirements:</p>
<ol type="1">
<li>To estimate our function from a wide variety of functions – roughly, we want to be able to estimate the relationship no matter how complicated it might be.</li>
<li>To estimate our function using a limited number of examples.</li>
</ol>
</section>
<section id="major-classes-of-ml-problems" class="level2">
<h2><span class="header-section-number">2.2</span> Major Classes of ML Problems</h2>
<ul>
<li><strong>Classification/Pattern Recognition</strong></li>
</ul>
<p>Given a set of observations, learn a generalizable rule that allows their discrete class to be predicted. For example, we see a series of animals with either two or four legs who do or don’t bark. We want to learn to categorize animals with two legs as humans, animals with four legs who don’t bark as cats and animals with four legs who do bark as dogs. We want to <em>partition</em> the input space.</p>
<p><img src="files/classification.png" /></p>
<ul>
<li><strong>Regression</strong></li>
</ul>
<p>Given a set of observations, learn a generalizable relationship that allows some real-valued output to be predicted. For example, we want to estimate a likely temperature after seeing a series of temperatures, times, humidities, air pressures, etc.</p>
<p><img src="files/regression.png" /></p>
<ul>
<li><strong>Density Estimation</strong></li>
</ul>
<p>Given a set of observations, we want to describe the probability distribution of the underlying population. The most commonly associated machine learning methods are those of cluster analysis, which seek to characterize the structure of data without knowing about category membership in advance – things that share a probability distribution are more likely to be from the same phenomenon than those that do not.</p>
</section>
<section id="a-brief-history" class="level2">
<h2><span class="header-section-number">2.3</span> A Brief History…</h2>
<p>(taken largely from Ch. 1 of Vapnik’s Statistical Learning Theory)</p>
<ul>
<li><p><strong>The parametric approach of the 1920-30’s</strong> - The classical, maximum likelihood methods developed largely by Fisher in the 1920’s pose the problem of statistical inference as 1) assuming the structure of the process that generated observed values – eg. the assumption of normality – in order to 2) estimate the parameters of a predefined function. This parametric method of inference works well when 1) the laws that generate the random error of observation and 2) the form of the function whose parameters are to be estimated are both known in advance. The number of problems that satisfy those two conditions are very few indeed. This approach reached its “golden age” from 1930-1960, but remains in common currency in some part by historical accident (its alternative was developed by the Russians during the Cold War) and in some part by many scientists being unwilling to give half an ass to learn math.</p></li>
<li><p><strong>Empirical Risk Minimization in the 1960’s</strong> - It is easy to forget that many of the classical methods were developed without computers, and evaluating complex datasets was laborious such that it neared impossibility. When we got our hands on our computers it became clear that the ideas that underlie the parametric approach to inference were naive, however convenient. It is from this realization that the practice of “data analysis” that seeks to describe data rather than make formal statistical inductions followed. The development of the <a href="https://en.wikipedia.org/wiki/Perceptron">perceptron</a>, which mimics very crude neurons that update input “weights” depending on repeated input to estimate category labels prompted the development of <em>Emperical Risk Minimization</em>. The ERM principle suggests that functions that generalize well to future inputs, ie. functions that describe the input/output relationship well, should be trained by minimizing the observed error (risk) on training examples. The major intellectual developments that followed were the description of the conditions for <em>consistency</em> of ERM – when ERM will arrive at the best possible solution – and the <em>quality</em> of the functions fit by ERM.</p></li>
<li><p><strong>Backpropagation in the 1980’s</strong> - This practice remained unpopular due to the limitations of perceptions and related learning algorithms, but the development of backpropagation that allowed multiple instances of a learning algorithm to be chained together renewed interest. Backpropagation allows the observed error at the output of a chain of stacked algorithms to be… propagated… backwards… through to the input, so that all weights or parameters of the function estimator can be intelligibly updated rather than just those of the output.</p></li>
<li><p><strong>Deep Learning in the 2000’s</strong> - Backpropagation is great, but computationally expensive. The advent of cheap GPUs that allow the matrix algebra necessary for deep algorithms to be performed in a tractable period of time.</p></li>
</ul>
<p>From this history we can see the major components of the machine learning approach:</p>
<ol type="1">
<li><p>Models are evaluated by a loss value: the difference between the correct answer and the predicted answer</p></li>
<li><p>Models are improved by updating their parameters to minimize that loss value</p></li>
<li><p>Multiple models, or units of models, are combined so increase the generality of the functions that can be estimated.</p></li>
<li><p>Models are tested by their ability to generalize to novel data, or by ‘cross-validation’</p></li>
</ol>
</section>
<section id="common-ml-algorithms" class="level2">
<h2><span class="header-section-number">2.4</span> Common ML Algorithms</h2>
<p>Good god there are so many. The folks that put together scikit-learn for Python made this flowchart that gives a very high-level overview of the major types</p>
<p><img src="files/ml_map.png" /></p>
<section id="decision-trees" class="level3">
<h3><span class="header-section-number">2.4.1</span> <strong>Decision Trees</strong></h3>
<p>For this I defer to this lovely presentation: <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/" class="uri">http://www.r2d3.us/visual-intro-to-machine-learning-part-1/</a></p>
</section>
<section id="support-vector-machines" class="level3">
<h3><span class="header-section-number">2.4.2</span> <strong>Support Vector Machines</strong></h3>
<p>In the simplest example, we learn some line, plane, or hyperplane that best separates classes.</p>
<figure>
<img src="files/svn_planes.png" alt="H1 does not separate the classes. H2 does, but only with a small margin. H3 separates them with the maximum margin." /><figcaption>H1 does not separate the classes. H2 does, but only with a small margin. H3 separates them with the maximum margin.</figcaption>
</figure>
</section>
<section id="artificial-neural-networks" class="level3">
<h3><span class="header-section-number">2.4.3</span> <strong>Artificial Neural Networks</strong></h3>
<p>Models are composed of neurons, arranged in layers, with particular connectivity between (or within) layers. Neurons can be arbitrarily complex, from the simplest weight-only neuron to the hypercomplex LSTM neurons. Models also get their power from their architecture, as exemplified convolutional neural networks.</p>
<figure>
<img src="files/an.jpg" alt="A neuron. Outputs are computed by multiplying inputs by trained weights, summing them, and passing that sum to some typically nonlinear function. http://www.theprojectspot.com/tutorial-post/introduction-to-artificial-neural-networks-part-1/7" /><figcaption>A neuron. Outputs are computed by multiplying inputs by trained weights, summing them, and passing that sum to some typically nonlinear function. <a href="http://www.theprojectspot.com/tutorial-post/introduction-to-artificial-neural-networks-part-1/7" class="uri">http://www.theprojectspot.com/tutorial-post/introduction-to-artificial-neural-networks-part-1/7</a></figcaption>
</figure>
<figure>
<img src="files/ANN-Diagram.png" alt="A simple multilayer, fully-connected ANN" /><figcaption>A simple multilayer, fully-connected ANN</figcaption>
</figure>
<figure>
<img src="files/Typical_cnn.png" alt="Convolutional Neural Nets learn simplified features by restricting their connectivity" /><figcaption>Convolutional Neural Nets learn simplified features by restricting their connectivity</figcaption>
</figure>
<p>And of course we have to <a href="https://www.youtube.com/watch?v=SCE-QeDfXtA">deep dream a little bit</a></p>
</section>
<section id="k-means-clustering" class="level3">
<h3><span class="header-section-number">2.4.4</span> <strong>k-Means Clustering</strong></h3>
<p>Divide data into k clusters such that each observation is identified as the cluster with the nearest mean value. One of the simplest clustering algorithms.</p>
<p><img src="files/K-means_convergence.gif" /></p>
</section>
</section>
<section id="resources" class="level2">
<h2><span class="header-section-number">2.5</span> Resources</h2>
<ul>
<li><p><a href="https://mitpress.mit.edu/books/foundations-machine-learning">Foundations of Machine Learning</a></p></li>
<li><p><a href="http://math.arizona.edu/~hzhang/math574m/Read/vapnik.pdf">Vapnik 1999 - An Overview of Statistical Learning Theory</a></p></li>
<li><p><a href="https://link.springer.com/chapter/10.1007/978-1-4757-2440-0_1">Vapnik - The Nature of Statistical Learning Theory</a></p></li>
<li><p><a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/">A tour of ML algorithms</a></p></li>
</ul>

</section>
</section>
<section id="regularized-regression-in-caret" class="level1">
<h1><span class="header-section-number">3</span> Regularized Regression in Caret</h1>
<p>This is intended to provide a (very brief) introduction to some key concepts in Machine Learning/Predictive Modeling, and how they work within a regression context. Regularized regression is a nice place for folks with a psych background to start, because its an extension of the familiar regression models we’ve all come to know and love.</p>
<p>Regression models, of course, are used when you have a <em>continuous</em> outcome. So many of our problems in psych involve continuous outcomes (e.g., personality dimensions, emotions/affect, etc.), and so regression models are pretty useful for psychologists.</p>
<p>But, before we dive into regularized regression, we should first cover some basic concepts in machine learning.</p>
<section id="the-bias-variance-tradeoff-overfitting-and-cross-validation" class="level2">
<h2><span class="header-section-number">3.1</span> The Bias-Variance Tradeoff, Overfitting, and Cross-Validation</h2>
<section id="bias-variance-tradeoff" class="level3">
<h3><span class="header-section-number">3.1.1</span> Bias-Variance Tradeoff</h3>
<p>This came up briefly last week, but the bias-variance tradeoff is really crucial to everything we’ll talk about today. The basic idea is that you can partition error into two components: bias and variance. <strong>Bias</strong> refers to the extent to which a model produces parameter estimates that miss in a particular direction (e.g., consistently over-estimating or consistently under-estimating a parameter value). <strong>Variance</strong> refers to the extent to which a model produces parameter estimates that vary from their central tendency across different datasets.</p>
<p>There is a tradeoff between bias and variance: all else equal, increasing bias will decrease variance. The basic idea here is that if we have a zero-bias estimator, it will tend to try to fit everything in the data, whereas an estimator biased in some direction won’t. This isn’t to say that bias is good or bad, just that there are times where we might want to increase bias to decrease variance. As we’ll see later, regularization is basically one method for introducing bias (to minimize variance).</p>
</section>
<section id="overfitting" class="level3">
<h3><span class="header-section-number">3.1.2</span> Overfitting</h3>
<p>This is probably familiar to folks in here (and it came up last week), so I won’t say much about it here. The basic idea is that any data has signal and noise. Sometimes, something appears to be signal but is actually noise. That is, when our statistical models are searching for the best solution, they sometimes will be fooled into thinking some noise is signal. This is usually called <strong>overfitting</strong>, and it has presented a pretty substantial problem in statistical modeling. As you’ll see, one of our goals is to try to avoid overfitting. Also worth noting is that overfitting will tend to produce a model with high variance, because noise will vary from dataset to dataset (basically by definition), and so a model which has fit noise will not do well across different datasets (with different noise).</p>
</section>
<section id="cross-validation" class="level3">
<h3><span class="header-section-number">3.1.3</span> Cross-Validation</h3>
<p>Cross-validation generally refers to taking a model that you trained on some data and using it in a new dataset. Unlike a replication, the model parameters carry over from the training to the test data (i.e., you don’t simply use the same variables and re-estimate the model parameters; you save the model parameters, and use it to predict the outcome variable). You can use cross-validation both to train and evaluate a model. A simple example may make this clear.</p>
<p>Let’s say we think home size (in square-feet) is the only relevant predictor for house price. So, we have some data on prices of recently sold houses, and estimate a model predicting house price from square-feet:</p>
<p><span class="math display">\[\hat{price_{i}} = b_0 + b_1*sqaurefeet_i\]</span></p>
<p>Let’s say we get these parameter values:</p>
<p><span class="math display">\[\hat{price_{i}} = 100 + 50*sqaurefeet_i\]</span></p>
<p>And now we want to cross-validate in a hold-out sample. We wouldn’t simply estimate this model again:</p>
<p><span class="math display">\[\hat{price_{i}} = b_0 + b_1*sqaurefeet_i\]</span></p>
<p>We would instead apply this model:</p>
<p><span class="math display">\[\hat{price_{i}} = 100 + 50*sqaurefeet\]</span> And evaluate how well it did. We could do this either by how much it misses, which is usually done with root mean squared error (RMSE). This is the average squared difference between observed (<span class="math inline">\(y_i\)</span>) and expected (<span class="math inline">\(\hat{y}_i\)</span>) values:</p>
<p><span class="math display">\[MSE = \frac{1}{n}\sum\limits_{i=1}^{n}(y_i - \hat{y_{i}})^2\]</span></p>
<p>And in our example, the y variable is house price:</p>
<p><span class="math display">\[MSE = \frac{1}{n}\sum\limits_{i=1}^{n}(price_i - \hat{price_{i}})^2\]</span> And then finally, we take the square root for RMSE:</p>
<p><span class="math display">\[RMSE = \sqrt{MSE}\]</span></p>
<p>Typically, people will also look at prediction accuracy, using the model’s <span class="math inline">\(R^2\)</span>. This is interpreted the same way as <span class="math inline">\(R^2\)</span> always is (as the % of variance in the outcome accounted for by the model).</p>
<section id="k-fold-cross-validation" class="level4">
<h4><span class="header-section-number">3.1.3.1</span> K-fold cross-validation</h4>
<p>There are different varieties of cross-validation. The most intuitive version is to create a single partition of data (i.e., split full data frame into two dataframes: training and test). However, there are other methods for cross-validation. One that has been gaining steam (or is maybe already at full steam at this point) is <strong>k-fold cross-validation</strong>. The basic idea is that we split a dataset into k subsamples (called folds). We then treat one subsample as the holdout sample, train on the remaining subsamples, and cross-validate on the holdout sample; then rinse and repeat so to speak. An example will probably help here.</p>
<p>Let’s take a ridiculously simple example (based on the earlier example). We want to predict house sale price from square footage:</p>
<p><span class="math display">\[\hat{price_i} = b_0 + b_1*sqaurefeet_i\]</span></p>
<p>Let’s say we have just 30 cases, and we use 10-fold cross validation. Let each observation be indicated by <span class="math inline">\(o_i\)</span>, so the first observation is <span class="math inline">\(o_1\)</span>, the second is <span class="math inline">\(o_2\)</span>, and the third is <span class="math inline">\(o_3\)</span>, etc.</p>
<p>First, we would fit a model using folds 2 through 10 (i.e., <span class="math inline">\(o_4\)</span> to <span class="math inline">\(o_30\)</span>), and then test it on the first fold (<span class="math inline">\(o_1\)</span> to <span class="math inline">\(o_3\)</span>). Then, we would fit the model using folds 1 and 3-10 (i.e., <span class="math inline">\(o_1\)</span> to <span class="math inline">\(o_3\)</span> &amp; <span class="math inline">\(o_7\)</span> to <span class="math inline">\(o_30\)</span>) and test it on the 2nd fold (<span class="math inline">\(o_4\)</span>, <span class="math inline">\(o_5\)</span>, <span class="math inline">\(o_6\)</span>), and so on until each fold was used as the holdout sample.</p>
<p>Then, we calculate the average performance across all of the tests.</p>
<p>Note that you can also use k-fold cross-validation for training purposes. Basically, this works by taking the best fitting model from a k-fold cross-validation procedure, and then testing it on a new holdout sample.</p>
</section>
</section>
</section>
<section id="regularization" class="level2">
<h2><span class="header-section-number">3.2</span> Regularization</h2>
<p>Now let’s get to regularized regression. This is a pretty simple extension of OLS regression. The logic of it is basically that OLS regression is minimally biased, but because of this, is higher variance than we might want. So, the solution is to introduce some bias into the model that will decrease variance. This takes the form of a new <em>penalization</em>, which tends to either be focused on parameter size, number of parameters, or both. Let’s start with the first. I find it helpful to think of these as having different beliefs, and choosing one depends on whether or not those beliefs seem correct.</p>
</section>
<section id="ridge-all-of-these-features-matter-but-only-a-little-bit." class="level2">
<h2><span class="header-section-number">3.3</span> Ridge: all of these features matter, but only a little bit.</h2>
<p>Ridge regression is basically OLS regression with an extra term. As a refresher, OLS seeks to minimize the sum of squared error, or:</p>
<p><span class="math display">\[SSE = \sum\limits_{i=1}^n (y_i - \hat{y_i})^2\]</span></p>
<p>Ridge adds an additional penalty:</p>
<p><span class="math display">\[SSE_{L2} = \sum\limits_{i=1}^n (y_i - \hat{y_i}^2) + \lambda \sum\limits_{j=1}^p \beta^2_j\]</span> This makes it so that paramter values are only allowed to be large if they reduce error enough to justify their size. Functionally, this makes it so parameter values shrink towards 0. You can hopefully see this in that as our paramater values (our betas) increase in size, error increases, since we are adding the sum of squared beta values, times some constant <span class="math inline">\(\lambda\)</span>, to our error term SSE. So, unless the parameter values decrease the first part of the error term (the ordinary sum of squared error; to the left of our new penalty) proportionally to their magnitude, they are shrunk toward 0.</p>
<p>The extent to which they are shrunk towards 0 depends on the value of <span class="math inline">\(\lambda\)</span>; higher values lead to more shrinkage than lower values. This is called a <em>hyperparameter</em> because it’s a parameter that governs other parameters. You can think of <span class="math inline">\(\lambda\)</span> as sort of the cost associated with larger parameter values: higher values of lambda are like telling your model that larger parameter values are more costly (so don’t make them large for nothing).</p>
<p>You can think of this penalty as introducing a specific type of bias: bias towards smaller parameter values. However, since larger parameter values can result from overfitting, this bias can result in reducing variance.</p>
<p>So why does Ridge do that and why is it useful? As I said earlier, I find it useful to think of statistical tools as having certain beliefs, and as being useful when those beliefs seem more or less true (in some particular case). Ridge believes that all of the variables you’re considering matter, but that most of them matter very little. Put differently, it believes that each variable you’ve entered belongs in the model, but that most or all only have small contributions. Because of this, people often say that ridge doesn’t perform <em>feature selection</em>, and shouldn’t be used if you need to select features (i.e., variables). This makes sense once you think of what Ridge believes: it believes every variable you’re telling it to use should be in the model, but many will simply have small impacts. If we want to select features (i.e., decide what variables go in our model), we need a different tool with a different set of beliefs.</p>
</section>
<section id="lasso-only-some-features-matter-and-they-might-matter-a-lot" class="level2">
<h2><span class="header-section-number">3.4</span> Lasso: only some features matter, and they might matter a lot</h2>
<p>Another popular form of regularized regression is the <em>least absolute shrinkage and selection operator</em> model, or <em>lasso</em>. Unlike ridge, lasso’s regularization simultaneously performs feature selection and model improvement.</p>
<p>Just like ridge, lasso is essentially our old friend OLS regression with an extra term added to error, which penalizes non-zero parameter values:</p>
<p><span class="math display">\[SSE_{L1} = \sum\limits_{i=1}^n = (y_i - \hat{y_i}^2) + \lambda \sum\limits_{j=1}^p |\beta_j|\]</span></p>
<p>It’s sort of hard (at least for me) to have a strong intuition about why this simple change leads to a model that functions differently. But, the basic idea is that penalizing the absolute value leads to some parameters actually being set to zero; the idea (I think) is that penalizing the absolute value leads to small departures from zero (e.g., .1) to be relatively more penalized than when you’re penalizing the squared value (since squaring a value &lt; 1 leads to a smaller value than its absolute value). This is most consequential for correlated predictors: Ridge will allow each of k correlated predictors to basically share the predictive duty, whereas Lasso will tend to pick the best and ignore the rest. SO, just like with ridge, lasso introduces bias, and its bias is that many predictors will have no relation to the outcome variable (i.e., only some features matter).</p>
<p>Let’s walk through an example with correlated predictors that I think will help. Let’s say we have an outcome <span class="math inline">\(Y\)</span>, and two predictors <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. And let’s imagine <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are highly correlated (<span class="math inline">\(r_{X_1, X_2} = .90\)</span>). Let’s say a model (Model 1) that contains predictors gives us this solution:</p>
<p><span class="math display">\[Model 1: y_i = .40*X_1 + .40*X_2\]</span> According to the path algebra, including just one of these predictors, <span class="math inline">\(X_1\)</span>, in the model would give us the following:</p>
<p><span class="math display">\[Model 2: y_i = .76*X_1\]</span></p>
<p>Note, this is just the path from <span class="math inline">\(X_2\)</span> to <span class="math inline">\(Y\)</span> (.40) times the correlation between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> (.90). So how would each of these penalties treat this? Let’s walk through it:</p>
<p>First, let’s simulate some data that has the properties we just mentioned. We’ll do this with the <code>mvtnorm</code> library. This allows us to take a random sample from a multivariate normal distribution. It requires a sample size, a vector of means (equal to the number of variables), and a variance-covariance matrix (called sigma; where r = c = number of variables). Since we’re talking about standardized solutions, we’ll create a variance-covariance (sigma) matrix that is standardized (i.e., a correlation matrix), with 1’s along the diaganol. We then specify the bivariate correlation between each variable, which will be .9 for X1 and X2, and then .76 for X1’s relation with Y and .76 for X2’s relation with Y (just like above). That looks something like this:</p>
<pre class="sourceCode r" id="cb1"><code class="sourceCode r"><div class="sourceLine" id="cb1-1" data-line-number="1"><span class="co"># Load the mvtnorm library</span></div>
<div class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">library</span>(mvtnorm)</div>
<div class="sourceLine" id="cb1-3" data-line-number="3"></div>
<div class="sourceLine" id="cb1-4" data-line-number="4"><span class="co"># specify sigma matrix;</span></div>
<div class="sourceLine" id="cb1-5" data-line-number="5"><span class="co"># again, this is the correlation matrix for the variables</span></div>
<div class="sourceLine" id="cb1-6" data-line-number="6"><span class="co"># since we&#39;re working with standardized values.</span></div>
<div class="sourceLine" id="cb1-7" data-line-number="7">sigma &lt;-<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">.9</span>, <span class="fl">.76</span>,</div>
<div class="sourceLine" id="cb1-8" data-line-number="8">                 <span class="fl">.9</span>, <span class="dv">1</span>, <span class="fl">.76</span>,</div>
<div class="sourceLine" id="cb1-9" data-line-number="9">                 <span class="fl">.76</span>, <span class="fl">.76</span>, <span class="dv">1</span>), <span class="dt">ncol =</span> <span class="dv">3</span>)</div>
<div class="sourceLine" id="cb1-10" data-line-number="10"></div>
<div class="sourceLine" id="cb1-11" data-line-number="11"><span class="co"># Now take the sample, call it sample_data</span></div>
<div class="sourceLine" id="cb1-12" data-line-number="12">sample_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">rmvnorm</span>(<span class="dt">n =</span> <span class="dv">1000000</span>, <span class="dt">mean =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">sigma =</span> sigma))</div>
<div class="sourceLine" id="cb1-13" data-line-number="13"></div>
<div class="sourceLine" id="cb1-14" data-line-number="14"><span class="co"># give the columns names; we&#39;ll use x1, x2, and y just like the example above</span></div>
<div class="sourceLine" id="cb1-15" data-line-number="15"><span class="kw">names</span>(sample_data) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x2&quot;</span>, <span class="st">&quot;y&quot;</span>)</div></code></pre>
<p>Alright, let’s check the correlation matrix to make sure we did this correctly. This should match sigma:</p>
<pre class="sourceCode r" id="cb2"><code class="sourceCode r"><div class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">cor</span>(sample_data)</div></code></pre>
<pre><code>##           x1        x2         y
## x1 1.0000000 0.8997763 0.7594485
## x2 0.8997763 1.0000000 0.7591536
## y  0.7594485 0.7591536 1.0000000</code></pre>
<p>Ah, it does! Good, let’s proceed. Now if we estimate, a regression with both variables, we should get two beta weights of about .40:</p>
<pre class="sourceCode r" id="cb4"><code class="sourceCode r"><div class="sourceLine" id="cb4-1" data-line-number="1">model_<span class="dv">1</span> &lt;-<span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> sample_data)</div>
<div class="sourceLine" id="cb4-2" data-line-number="2"><span class="kw">summary</span>(model_<span class="dv">1</span>)</div></code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2, data = sample_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.1681 -0.4241  0.0003  0.4231  2.9531 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.0008660  0.0006272  -1.381    0.167    
## x1           0.4016308  0.0014385 279.204   &lt;2e-16 ***
## x2           0.3987393  0.0014387 277.156   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6272 on 999997 degrees of freedom
## Multiple R-squared:  0.607,  Adjusted R-squared:  0.607 
## F-statistic: 7.721e+05 on 2 and 999997 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Okay, that worked as expected; we get two beta weights of about .40 (if you round to 2 decimals). Now let’s check model 2, where we just include 1 x variable (x1). We should get a single beta weight of about .76.</p>
<pre class="sourceCode r" id="cb6"><code class="sourceCode r"><div class="sourceLine" id="cb6-1" data-line-number="1">model_<span class="dv">2</span> &lt;-<span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1, <span class="dt">data =</span> sample_data)</div>
<div class="sourceLine" id="cb6-2" data-line-number="2"><span class="kw">summary</span>(model_<span class="dv">2</span>)</div></code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x1, data = sample_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4964 -0.4389 -0.0001  0.4386  3.2230 
## 
## Coefficients:
##               Estimate Std. Error  t value Pr(&gt;|t|)    
## (Intercept) -0.0007002  0.0006508   -1.076    0.282    
## x1           0.7603576  0.0006513 1167.362   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6508 on 999998 degrees of freedom
## Multiple R-squared:  0.5768, Adjusted R-squared:  0.5768 
## F-statistic: 1.363e+06 on 1 and 999998 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>And we do. Now let’s walk through the two penalties we covered so far, Ridge and Lasso. Based on what we know so far, Ridge should prefer Model 1 (with x1 and x2) and LASSO should prefer model 2 (the one with just x1)</p>
<pre class="sourceCode r" id="cb8"><code class="sourceCode r"><div class="sourceLine" id="cb8-1" data-line-number="1"><span class="co"># pull out the coefficients for each parameter of the two models</span></div>
<div class="sourceLine" id="cb8-2" data-line-number="2">model_<span class="dv">1</span>_b1 &lt;-<span class="st"> </span>model_<span class="dv">1</span><span class="op">$</span>coefficients[<span class="st">&quot;x1&quot;</span>]</div>
<div class="sourceLine" id="cb8-3" data-line-number="3">model_<span class="dv">1</span>_b2 &lt;-<span class="st"> </span>model_<span class="dv">1</span><span class="op">$</span>coefficients[<span class="st">&quot;x2&quot;</span>]</div>
<div class="sourceLine" id="cb8-4" data-line-number="4">model_<span class="dv">2</span>_b1 &lt;-<span class="st"> </span>model_<span class="dv">2</span><span class="op">$</span>coefficients[<span class="st">&quot;x1&quot;</span>]</div>
<div class="sourceLine" id="cb8-5" data-line-number="5"><span class="co"># Note, we&#39;re not actually estimating b2 in model 2, but this is equivalen to saying its 0</span></div>
<div class="sourceLine" id="cb8-6" data-line-number="6">model_<span class="dv">2</span>_b2 &lt;-<span class="st"> </span><span class="dv">0</span></div>
<div class="sourceLine" id="cb8-7" data-line-number="7"></div>
<div class="sourceLine" id="cb8-8" data-line-number="8"><span class="co"># set lambda to a constant; we&#39;ll use .1</span></div>
<div class="sourceLine" id="cb8-9" data-line-number="9">lambda &lt;-<span class="st"> </span><span class="fl">.1</span></div>
<div class="sourceLine" id="cb8-10" data-line-number="10">ridge_penalty_Model_<span class="dv">1</span> &lt;-<span class="st"> </span>lambda<span class="op">*</span>(<span class="kw">sum</span>(<span class="kw">c</span>(model_<span class="dv">1</span>_b1<span class="op">^</span><span class="dv">2</span>, model_<span class="dv">1</span>_b2<span class="op">^</span><span class="dv">2</span>)))</div>
<div class="sourceLine" id="cb8-11" data-line-number="11">ridge_penalty_Model_<span class="dv">2</span> &lt;-<span class="st"> </span>lambda<span class="op">*</span>(<span class="kw">sum</span>(<span class="kw">c</span>(model_<span class="dv">2</span>_b1<span class="op">^</span><span class="dv">2</span>, model_<span class="dv">2</span>_b2<span class="op">^</span><span class="dv">2</span>)))</div>
<div class="sourceLine" id="cb8-12" data-line-number="12"></div>
<div class="sourceLine" id="cb8-13" data-line-number="13">ridge_penalties &lt;-<span class="st"> </span><span class="kw">rbind</span>(ridge_penalty_Model_<span class="dv">1</span>, ridge_penalty_Model_<span class="dv">2</span>)</div>
<div class="sourceLine" id="cb8-14" data-line-number="14"></div>
<div class="sourceLine" id="cb8-15" data-line-number="15">lasso_penalty_Model_<span class="dv">1</span> &lt;-<span class="st"> </span>lambda<span class="op">*</span>(<span class="kw">sum</span>(<span class="kw">c</span>(<span class="kw">abs</span>(model_<span class="dv">1</span>_b1), <span class="kw">abs</span>(model_<span class="dv">1</span>_b2))))</div>
<div class="sourceLine" id="cb8-16" data-line-number="16">lasso_penalty_Model_<span class="dv">2</span> &lt;-<span class="st"> </span>lambda<span class="op">*</span>(<span class="kw">sum</span>(<span class="kw">c</span>(<span class="kw">abs</span>(model_<span class="dv">2</span>_b1), <span class="kw">abs</span>(model_<span class="dv">2</span>_b2))))</div>
<div class="sourceLine" id="cb8-17" data-line-number="17"></div>
<div class="sourceLine" id="cb8-18" data-line-number="18">lasso_penalties &lt;-<span class="st"> </span><span class="kw">rbind</span>(lasso_penalty_Model_<span class="dv">1</span>, lasso_penalty_Model_<span class="dv">2</span>)</div>
<div class="sourceLine" id="cb8-19" data-line-number="19">penalties &lt;-<span class="st"> </span><span class="kw">cbind</span>(ridge_penalties, lasso_penalties)</div>
<div class="sourceLine" id="cb8-20" data-line-number="20"></div>
<div class="sourceLine" id="cb8-21" data-line-number="21"><span class="kw">colnames</span>(penalties)&lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ridge&quot;</span>, <span class="st">&quot;lasso&quot;</span>)</div>
<div class="sourceLine" id="cb8-22" data-line-number="22"><span class="kw">row.names</span>(penalties) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;model 1&quot;</span>, <span class="st">&quot;model 2&quot;</span>)</div>
<div class="sourceLine" id="cb8-23" data-line-number="23">knitr<span class="op">::</span><span class="kw">kable</span>(penalties, <span class="dt">digits =</span> <span class="dv">3</span>)</div></code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">ridge</th>
<th style="text-align: right;">lasso</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>model 1</td>
<td style="text-align: right;">0.032</td>
<td style="text-align: right;">0.080</td>
</tr>
<tr class="even">
<td>model 2</td>
<td style="text-align: right;">0.058</td>
<td style="text-align: right;">0.076</td>
</tr>
</tbody>
</table>
<p>Now what you could hopefully see there is that, all else equal, lasso prefers fewer predictors (which can have larger values) than ridge. How much it penalizes predictors depends again on <span class="math inline">\(\lambda\)</span>, which again is a <em>hyperparameter</em>.</p>
<p>So returning to why we would use it, it’s easiest for me to see when it would be useful by thinking about what Lasso believes: it believes that non-zero predictors are costly (and cost doesn’t accelerate with parameter value size, like ridge does). It (sort of) believes that only some of the variables are needed, and the ones that are needed can take on relatively larger sizes.</p>
<p>What if our belief is somewhere in between these options: that some variables may not be needed (may actually be zero), but that many of the variables should have smaller values?</p>
</section>
<section id="elastic-net-maybe-everything-matters-and-maybe-only-a-little-bit." class="level2">
<h2><span class="header-section-number">3.5</span> Elastic Net: maybe everything matters, and maybe only a little bit.</h2>
<p>Elastic net combines the penalties used by ridge and lasso. In doing so, it basically takes the middle ground between these two methods: penalizing non-zero values (feature selection) and penalizing values the further they depart from zero (regularization). So now, our error has three terms:</p>
<ol type="1">
<li>sum of squared errors</li>
<li>ridge penalty</li>
<li>lasso penalty</li>
</ol>
<p>The formula for this error term is:</p>
<p><span class="math display">\[SSE_{Enet} = \sum\limits_{i=1}^n  (y_i - \hat{y_i}^2) + \lambda_1 \sum\limits_{j=1}^p \beta^2_j + \lambda_2 \sum\limits_{j=1}^p |\beta_j|\]</span></p>
<p>Basically, elastic net is sort of a best of both worlds approach: it gives you the feature selection of lasso, and regularizes as effective as ridge. It thus introduces two dimensions of bias:</p>
<ol type="1">
<li>that most predictors have small relations to the outcome.</li>
<li>that many predicotrs have no relation to the outcome.</li>
</ol>
<p>How much each is priortized depends on the sie of <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> respectively. It’s often a great place to start, because as you’re tuning the hyperparameters, you can get to one of the other methods if that is truly the best method. For example, if lasso is actually the best method for your data, then (if your training is working well) you should end up with a zero value for <span class="math inline">\(\lambda_1\)</span>, leading to the ridge penalty dropping out of the model (and leaving you with a lasso model). However, in my limited experience, it usually ends up with some non-zero value for both (which I think says something about the problems we deal with).</p>
<p>In terms of beliefs, Elastic net is basically a more flexible thinker: it thinks we might only need few predictors and that each predictor may only contribute a little bit, and its willing to weigh these things more or less depending on what works better (either determined a priori, or determined via training).</p>
<p>Okay, this has been a (very brief) intro to regularized regression and some foundational concepts in machine learning necessary to understand it.</p>
</section>
<section id="example-using-caret" class="level2">
<h2><span class="header-section-number">3.6</span> Example using Caret</h2>
<p>Now let’s walk through an example:</p>
<p>We’re going to work with this data on wine reviews. It has the score it received in a rating, as well as some data about the wine, and a description of the wine. We’ll see how well we can predict the rating based on the data about the wine (including the description). I found this on &lt;kaggle.com&gt;</p>
<pre class="sourceCode r" id="cb9"><code class="sourceCode r"><div class="sourceLine" id="cb9-1" data-line-number="1"><span class="co"># clear the environment, just to be safe</span></div>
<div class="sourceLine" id="cb9-2" data-line-number="2"><span class="kw">rm</span>(<span class="dt">list=</span><span class="kw">ls</span>())</div>
<div class="sourceLine" id="cb9-3" data-line-number="3"></div>
<div class="sourceLine" id="cb9-4" data-line-number="4"><span class="kw">library</span>(caret)</div></code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre class="sourceCode r" id="cb12"><code class="sourceCode r"><div class="sourceLine" id="cb12-1" data-line-number="1"><span class="kw">library</span>(tidyverse)</div></code></pre>
<pre><code>## Loading tidyverse: tibble
## Loading tidyverse: tidyr
## Loading tidyverse: readr
## Loading tidyverse: purrr
## Loading tidyverse: dplyr</code></pre>
<pre><code>## Conflicts with tidy packages ----------------------------------------------</code></pre>
<pre><code>## filter(): dplyr, stats
## lag():    dplyr, stats
## lift():   purrr, caret</code></pre>
<pre class="sourceCode r" id="cb16"><code class="sourceCode r"><div class="sourceLine" id="cb16-1" data-line-number="1"><span class="kw">library</span>(tidytext)</div>
<div class="sourceLine" id="cb16-2" data-line-number="2"><span class="kw">library</span>(topicmodels)</div>
<div class="sourceLine" id="cb16-3" data-line-number="3"><span class="kw">require</span>(janitor)</div></code></pre>
<pre><code>## Loading required package: janitor</code></pre>
<pre class="sourceCode r" id="cb18"><code class="sourceCode r"><div class="sourceLine" id="cb18-1" data-line-number="1"><span class="kw">require</span>(rio)</div></code></pre>
<pre><code>## Loading required package: rio</code></pre>
<pre class="sourceCode r" id="cb20"><code class="sourceCode r"><div class="sourceLine" id="cb20-1" data-line-number="1">wine &lt;-<span class="st"> </span>rio<span class="op">::</span><span class="kw">import</span>(<span class="st">&quot;files/winemag-data_first150k.csv&quot;</span>,</div>
<div class="sourceLine" id="cb20-2" data-line-number="2">                    <span class="dt">setclass =</span> <span class="st">&quot;tbl_df&quot;</span>) <span class="op">%&gt;%</span></div>
<div class="sourceLine" id="cb20-3" data-line-number="3"><span class="st">  </span>janitor<span class="op">::</span><span class="kw">clean_names</span>() <span class="op">%&gt;%</span></div>
<div class="sourceLine" id="cb20-4" data-line-number="4"><span class="st">  </span><span class="kw">rename</span>(<span class="dt">id =</span> v1)</div></code></pre>
<p>We’ll limit ourselves to a sample of 1000 observations for time’s sake.</p>
<pre class="sourceCode r" id="cb21"><code class="sourceCode r"><div class="sourceLine" id="cb21-1" data-line-number="1"><span class="co">#set.seed(227)</span></div>
<div class="sourceLine" id="cb21-2" data-line-number="2"></div>
<div class="sourceLine" id="cb21-3" data-line-number="3"><span class="co">#wine &lt;- sample_n(wine, 1000)</span></div></code></pre>
<p>Let’s take a look at the data</p>
<pre class="sourceCode r" id="cb22"><code class="sourceCode r"><div class="sourceLine" id="cb22-1" data-line-number="1">wine</div></code></pre>
<pre><code>## # A tibble: 150,930 x 11
##       id country
##    &lt;int&gt;   &lt;chr&gt;
##  1     0      US
##  2     1   Spain
##  3     2      US
##  4     3      US
##  5     4  France
##  6     5   Spain
##  7     6   Spain
##  8     7   Spain
##  9     8      US
## 10     9      US
## # ... with 150,920 more rows, and 9 more variables: description &lt;chr&gt;,
## #   designation &lt;chr&gt;, points &lt;int&gt;, price &lt;dbl&gt;, province &lt;chr&gt;,
## #   region_1 &lt;chr&gt;, region_2 &lt;chr&gt;, variety &lt;chr&gt;, winery &lt;chr&gt;</code></pre>
<p>So you can see wee have some information about where the wine is from, its rating (called <code>points</code>), its price called <code>price</code>, and textual description of the wine (called <code>description</code>). Let’s see if we can train a model that does a good job predicting wine ratings.</p>
<section id="extracting-features-a-brief-detour-into-text-analysis" class="level3">
<h3><span class="header-section-number">3.6.1</span> Extracting features: a brief detour into text analysis</h3>
<p>Okay, before we start down the modelling road, we want to do something with the textual descriptions. Along these lines, we’ll separate out the description, do some automated text analysis, and use the output of those analyses as features. This should make a bit more sense as we walk through.</p>
<p>First we need to load in a dataframe of stopwords these are basically words that don’t have content and that we don’t need (e.g., “the”, “a”, “and”, etc.). Luckily, the R package <code>tidytext</code> has some built-in data on stop words. We need to load these up with the <code>data()</code> command.</p>
<pre class="sourceCode r" id="cb24"><code class="sourceCode r"><div class="sourceLine" id="cb24-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;stop_words&quot;</span>)</div></code></pre>
<p>Now that we have the stop words, let’s take our wine data set, select just the id (for merging purposes) and the text descriptions. Then we’ll use the <code>unnest_tokens</code> function, which basically takes the descriptions, separates them by ‘tokens’ (which in this case is each word), and leaves us with a dataset with a row per word in each desciption (essentially an id X word from description lengthed dataset).</p>
<p>Finally, we’ll use <code>anti_join()</code> to remove the stop words. This takes requires two dataframes as its two arguments, and removes any rows from the dataframe in the first argument that are in the dataframe in the second argument. Since we’re using pipes (<code>%&gt;%</code>), the first argument is invisible, but is the new expanded wine description data, and the second is the dataframe of stopwords; effectively, this will just remove the stopwords from our expanded wine description data.</p>
<pre class="sourceCode r" id="cb25"><code class="sourceCode r"><div class="sourceLine" id="cb25-1" data-line-number="1">wine_text_expand&lt;-<span class="st"> </span>wine <span class="op">%&gt;%</span></div>
<div class="sourceLine" id="cb25-2" data-line-number="2"><span class="st">  </span><span class="co"># take just the id and description</span></div>
<div class="sourceLine" id="cb25-3" data-line-number="3"><span class="st">  </span><span class="kw">select</span>(id, description) <span class="op">%&gt;%</span></div>
<div class="sourceLine" id="cb25-4" data-line-number="4"><span class="st">  </span><span class="co"># unnest tokens; provide it the new variable name (for the tokens)</span></div>
<div class="sourceLine" id="cb25-5" data-line-number="5"><span class="st">  </span><span class="co"># and the old variable (where it can find the text to tokenize).</span></div>
<div class="sourceLine" id="cb25-6" data-line-number="6"><span class="st">  </span><span class="kw">unnest_tokens</span>(word, description) <span class="op">%&gt;%</span></div>
<div class="sourceLine" id="cb25-7" data-line-number="7"><span class="st">  </span><span class="kw">anti_join</span>(stop_words)</div></code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<p>Okay, now that we have cleaned up text data for the wine descriptions, let’s extract some features from the descriptions. We can use sentiment analysis. since that seems like it will definitely be relevant. Sentiment analysis is intended to extract the emotional tone of a text, and in this case, will basically give us a score corresponding to how poisitive and negative each word is. We’ll leave it at sentiment analysis for the sake of time.</p>
<p>We’ll do sentiment analysis using tidytext, and the “afinn” sentiment dictionary. This dictionary has a set of words with a continuous sentiment score (from -3, to +3; neutral point of 0). We can use it to get sentiment scores by using <code>inner_join()</code>, which basically keeps all columns, but only rows shared by the two dataframes; in this case, only words that are in both our descriptions data AND the sentiment dictionary will be kept, and columns for id, word, and sentiment score. Then, we’ll summarize across words to get a sentiment score for each wine’s description (wine being tracked with id). This will leave us with a dataframe with id and sentiment score (since the words are shared between the two dataframes)</p>
<pre class="sourceCode r" id="cb27"><code class="sourceCode r"><div class="sourceLine" id="cb27-1" data-line-number="1">description_sentiment &lt;-<span class="st"> </span>wine_text_expand <span class="op">%&gt;%</span></div>
<div class="sourceLine" id="cb27-2" data-line-number="2"><span class="st">  </span><span class="co"># this makes it so that all is saved is a</span></div>
<div class="sourceLine" id="cb27-3" data-line-number="3"><span class="st">  </span><span class="co"># data frame that contains the words in the afinn</span></div>
<div class="sourceLine" id="cb27-4" data-line-number="4"><span class="st">  </span><span class="co"># sentiment dictionary, the score associated with those words,</span></div>
<div class="sourceLine" id="cb27-5" data-line-number="5"><span class="st">  </span><span class="co"># and the id for the wine.</span></div>
<div class="sourceLine" id="cb27-6" data-line-number="6"><span class="st">  </span><span class="kw">inner_join</span>(<span class="kw">get_sentiments</span>(<span class="st">&quot;afinn&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></div>
<div class="sourceLine" id="cb27-7" data-line-number="7"><span class="st">  </span><span class="co"># group by wine id</span></div>
<div class="sourceLine" id="cb27-8" data-line-number="8"><span class="st">  </span><span class="kw">group_by</span>(id) <span class="op">%&gt;%</span><span class="st"> </span></div>
<div class="sourceLine" id="cb27-9" data-line-number="9"><span class="st">  </span><span class="co"># summarize such that we have a single sentiment score </span></div>
<div class="sourceLine" id="cb27-10" data-line-number="10"><span class="st">  </span><span class="co"># per wine id</span></div>
<div class="sourceLine" id="cb27-11" data-line-number="11"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">sentiment =</span> <span class="kw">mean</span>(score)) </div></code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="sourceCode r" id="cb29"><code class="sourceCode r"><div class="sourceLine" id="cb29-1" data-line-number="1">description_sentiment</div></code></pre>
<pre><code>## # A tibble: 123,785 x 2
##       id sentiment
##    &lt;int&gt;     &lt;dbl&gt;
##  1     0       1.8
##  2     1       1.5
##  3     3       2.0
##  4     4       2.0
##  5     5       2.5
##  6     6       2.5
##  7     7       2.0
##  8     9       0.0
##  9    10      -1.0
## 10    12       4.0
## # ... with 123,775 more rows</code></pre>
<p>And, let’s merge that back into the wine dataframe.</p>
<pre class="sourceCode r" id="cb31"><code class="sourceCode r"><div class="sourceLine" id="cb31-1" data-line-number="1">wine_for_ml &lt;-<span class="st"> </span>wine <span class="op">%&gt;%</span></div>
<div class="sourceLine" id="cb31-2" data-line-number="2"><span class="st">  </span><span class="kw">left_join</span>(description_sentiment, <span class="dt">by =</span> <span class="st">&quot;id&quot;</span>) <span class="op">%&gt;%</span></div>
<div class="sourceLine" id="cb31-3" data-line-number="3"><span class="st">  </span><span class="co"># removing raw description for now</span></div>
<div class="sourceLine" id="cb31-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(points, price, sentiment) <span class="op">%&gt;%</span></div>
<div class="sourceLine" id="cb31-5" data-line-number="5"><span class="st">  </span><span class="co"># just removing missing values, because they complicate things</span></div>
<div class="sourceLine" id="cb31-6" data-line-number="6"><span class="st">  </span><span class="kw">na.omit</span>()</div></code></pre>
</section>
<section id="modeling-with-caret" class="level3">
<h3><span class="header-section-number">3.6.2</span> Modeling with caret</h3>
<p>We have quite a bit of data here (150000 cases), so first let’s partition our data into a training and test dataframe. We’ll do a 75-25 training-test split, and can use caret’s <code>createDataPartition()</code> function to do it.</p>
<pre class="sourceCode r" id="cb32"><code class="sourceCode r"><div class="sourceLine" id="cb32-1" data-line-number="1"><span class="co"># Set seed for consistency&#39;s sake</span></div>
<div class="sourceLine" id="cb32-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">227</span>)</div>
<div class="sourceLine" id="cb32-3" data-line-number="3"><span class="co"># This part creates a list of values;</span></div>
<div class="sourceLine" id="cb32-4" data-line-number="4"><span class="co"># these values are the row numbers for data included in the training set</span></div>
<div class="sourceLine" id="cb32-5" data-line-number="5"><span class="co"># we&#39;re splitting it 75-25, such that 75% of cases will be in the training set (25% in the test).</span></div>
<div class="sourceLine" id="cb32-6" data-line-number="6">in_train &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> wine_for_ml<span class="op">$</span>points,</div>
<div class="sourceLine" id="cb32-7" data-line-number="7">                                 <span class="dt">p =</span> <span class="fl">.75</span>,</div>
<div class="sourceLine" id="cb32-8" data-line-number="8">                                 <span class="dt">list =</span> <span class="ot">FALSE</span>)</div>
<div class="sourceLine" id="cb32-9" data-line-number="9"><span class="co"># subsets the training data (those data whose row number appears in the inTrain object)</span></div>
<div class="sourceLine" id="cb32-10" data-line-number="10">training &lt;-<span class="st"> </span>wine_for_ml[in_train,]</div>
<div class="sourceLine" id="cb32-11" data-line-number="11"><span class="co"># subsets the test data (those data whose row number DOES NOT appear in the inTrain object)</span></div>
<div class="sourceLine" id="cb32-12" data-line-number="12">testing &lt;-<span class="st"> </span>wine_for_ml[<span class="op">-</span>in_train,]</div></code></pre>
<p>Okay, now that we have our training data, let’s actually train a model.</p>
<p>First, we set up the training parameters using the <code>trainControl()</code> function. This is where you specify the method (e.g., cross-validation), and some other specifics.</p>
<p>In this example, we’ll tell it to use 10-fold cross-validation, by specifying cross-validation as the method, and 10 as the number (i.e., k). We’ll also tell it to save the best fitting model with the argument <code>savePredictions = TRUE</code>. You’ll notice that we’ll use these same controls for the different models we try (ridge, lasso, elastic net)</p>
<pre class="sourceCode r" id="cb33"><code class="sourceCode r"><div class="sourceLine" id="cb33-1" data-line-number="1"><span class="co"># Sets parameters for training;</span></div>
<div class="sourceLine" id="cb33-2" data-line-number="2"><span class="co"># telling it to use 10-fold cross-validation, and to save the predictions.</span></div>
<div class="sourceLine" id="cb33-3" data-line-number="3">train_control&lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number=</span><span class="dv">10</span>, </div>
<div class="sourceLine" id="cb33-4" data-line-number="4">                             <span class="dt">savePredictions =</span> <span class="ot">TRUE</span>)</div></code></pre>
<section id="ridge-method" class="level4">
<h4><span class="header-section-number">3.6.2.1</span> Ridge Method</h4>
<p>Now let’s train a model using ridge regression. We do this by telling it to:</p>
<ol type="1">
<li>predict points from everything (including interactions). This is accomplished with .*. where ‘.’ = all (well, all except the outcome variable).</li>
<li>using the training data</li>
<li>using the training parameters we just set</li>
<li>using the ridge method</li>
<li>pre-processing by centering and scaling (essentially z scoring everything; this is critical, because we want everything on the same scale, since parameter size is being penalized in one way or another).</li>
</ol>
<pre class="sourceCode r" id="cb34"><code class="sourceCode r"><div class="sourceLine" id="cb34-1" data-line-number="1">fit_ridge &lt;-<span class="st"> </span><span class="kw">train</span>(points <span class="op">~</span><span class="st"> </span>.<span class="op">*</span>., </div>
<div class="sourceLine" id="cb34-2" data-line-number="2">                   <span class="dt">data =</span> training,</div>
<div class="sourceLine" id="cb34-3" data-line-number="3">                   <span class="dt">trControl =</span> train_control,</div>
<div class="sourceLine" id="cb34-4" data-line-number="4">                   <span class="dt">method =</span> <span class="st">&quot;ridge&quot;</span>,</div>
<div class="sourceLine" id="cb34-5" data-line-number="5">                   <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))</div></code></pre>
<pre><code>## Loading required package: elasticnet</code></pre>
<pre><code>## Loading required package: lars</code></pre>
<pre><code>## Loaded lars 1.2</code></pre>
<p>Okay, so how did our model do? We can evaluate this in two different ways given what we’ve done so far.</p>
<ol type="1">
<li>Easier test: what is the average fit (with <span class="math inline">\(R^2\)</span>) and misfit (with <span class="math inline">\(RMSE\)</span>) fromt the training. This will basically take the <span class="math inline">\(R^2\)</span> and <span class="math inline">\(RMSE\)</span> from all 10 folds and average them, telling us how well our models were doing on average across training runs.</li>
</ol>
<p>We can get this information like so:</p>
<pre class="sourceCode r" id="cb38"><code class="sourceCode r"><div class="sourceLine" id="cb38-1" data-line-number="1">avg_r2_ridge &lt;-<span class="st"> </span><span class="kw">mean</span>(fit_ridge<span class="op">$</span>results<span class="op">$</span>Rsquared)</div>
<div class="sourceLine" id="cb38-2" data-line-number="2">avg_RMSE_ridge &lt;-<span class="st"> </span><span class="kw">mean</span>(fit_ridge<span class="op">$</span>results<span class="op">$</span>RMSE)</div>
<div class="sourceLine" id="cb38-3" data-line-number="3"></div>
<div class="sourceLine" id="cb38-4" data-line-number="4">avg_r2_ridge</div></code></pre>
<pre><code>## [1] 0.2686071</code></pre>
<pre class="sourceCode r" id="cb40"><code class="sourceCode r"><div class="sourceLine" id="cb40-1" data-line-number="1">avg_RMSE_ridge</div></code></pre>
<pre><code>## [1] 2.759382</code></pre>
<p>Okay, so an <span class="math inline">\(\bar{R^2}\)</span> of 0.27, meaning we are explaining 26.86% of the variance in wine ratings with sentiment and price (and the interaction).</p>
<ol start="2" type="1">
<li>Harder test: how well does it do with the holdout sample?</li>
</ol>
<pre class="sourceCode r" id="cb42"><code class="sourceCode r"><div class="sourceLine" id="cb42-1" data-line-number="1">pred_ridge &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_ridge, <span class="dt">newdata =</span> testing)</div>
<div class="sourceLine" id="cb42-2" data-line-number="2"></div>
<div class="sourceLine" id="cb42-3" data-line-number="3"><span class="co"># Gets R^2 and RMSE for ridge model</span></div>
<div class="sourceLine" id="cb42-4" data-line-number="4">fitstat_ridge &lt;-<span class="st"> </span><span class="kw">postResample</span>(<span class="dt">pred =</span> pred_ridge, </div>
<div class="sourceLine" id="cb42-5" data-line-number="5">                                  <span class="dt">obs =</span> testing<span class="op">$</span>points)</div>
<div class="sourceLine" id="cb42-6" data-line-number="6">fitstat_ridge</div></code></pre>
<pre><code>##      RMSE  Rsquared 
## 2.8348519 0.2411339</code></pre>
<p>Okay, so an <span class="math inline">\(R^2\)</span> of 0.24, meaning we are explaining 24.11% of the variance in wine ratings with sentiment and price (and the interaction).</p>
</section>
<section id="lasso-method" class="level4">
<h4><span class="header-section-number">3.6.2.2</span> Lasso Method</h4>
<p>Okay, now let’s try lasso. We’ll use the same training-test split, and the same training parameters.</p>
<p>We will run virtually the same code, but change <code>method = &quot;ridge&quot;</code> to <code>method = &quot;lasso&quot;</code>, like so:</p>
<pre class="sourceCode r" id="cb44"><code class="sourceCode r"><div class="sourceLine" id="cb44-1" data-line-number="1">fit_lasso &lt;-<span class="st"> </span><span class="kw">train</span>(points <span class="op">~</span><span class="st"> </span>.<span class="op">*</span>., </div>
<div class="sourceLine" id="cb44-2" data-line-number="2">                   <span class="dt">data =</span> training,</div>
<div class="sourceLine" id="cb44-3" data-line-number="3">                   <span class="dt">trControl =</span> train_control,</div>
<div class="sourceLine" id="cb44-4" data-line-number="4">                   <span class="dt">method =</span> <span class="st">&quot;lasso&quot;</span>,</div>
<div class="sourceLine" id="cb44-5" data-line-number="5">                   <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))</div></code></pre>
<p>And let’s evaluate this model in the same two ways.</p>
<p>Easier test: average fit across training runs:</p>
<pre class="sourceCode r" id="cb45"><code class="sourceCode r"><div class="sourceLine" id="cb45-1" data-line-number="1">avg_r2_lasso &lt;-<span class="st"> </span><span class="kw">mean</span>(fit_lasso<span class="op">$</span>results<span class="op">$</span>Rsquared)</div>
<div class="sourceLine" id="cb45-2" data-line-number="2">avg_RMSE_lasso &lt;-<span class="st"> </span><span class="kw">mean</span>(fit_lasso<span class="op">$</span>results<span class="op">$</span>RMSE)</div>
<div class="sourceLine" id="cb45-3" data-line-number="3"></div>
<div class="sourceLine" id="cb45-4" data-line-number="4">avg_r2_lasso</div></code></pre>
<pre><code>## [1] 0.249744</code></pre>
<pre class="sourceCode r" id="cb47"><code class="sourceCode r"><div class="sourceLine" id="cb47-1" data-line-number="1">avg_RMSE_lasso</div></code></pre>
<pre><code>## [1] 2.908199</code></pre>
<p>Okay, so an <span class="math inline">\(\bar{R^2}\)</span> of 0.25, meaning we are explaining 24.97% of the variance in wine ratings with sentiment and price (and the interaction).</p>
<p>Now for the harder test.</p>
<p>Harder test: how does it do with the holdout sample?</p>
<pre class="sourceCode r" id="cb49"><code class="sourceCode r"><div class="sourceLine" id="cb49-1" data-line-number="1">pred_lasso &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_lasso, <span class="dt">newdata =</span> testing)</div>
<div class="sourceLine" id="cb49-2" data-line-number="2"></div>
<div class="sourceLine" id="cb49-3" data-line-number="3"><span class="co"># Gets R^2 and RMSE for lasso model</span></div>
<div class="sourceLine" id="cb49-4" data-line-number="4">fitstat_lasso &lt;-<span class="st"> </span><span class="kw">postResample</span>(<span class="dt">pred =</span> pred_lasso, </div>
<div class="sourceLine" id="cb49-5" data-line-number="5">                                  <span class="dt">obs =</span> testing<span class="op">$</span>points)</div>
<div class="sourceLine" id="cb49-6" data-line-number="6"></div>
<div class="sourceLine" id="cb49-7" data-line-number="7">fitstat_lasso</div></code></pre>
<pre><code>##      RMSE  Rsquared 
## 2.8329419 0.2400375</code></pre>
<p>Okay, so an <span class="math inline">\(R^2\)</span> of 0.24, meaning we are explaining 24% of the variance in wine ratings with sentiment and price (and the interaction).</p>
</section>
<section id="elastic-net-method" class="level4">
<h4><span class="header-section-number">3.6.2.3</span> Elastic Net Method</h4>
<p>And finally, let’s do the same with elastic net. Like before, we’ll use the same data split and training parameters. And again, the code is <em>almost</em> identical; we just change <code>method = &quot;lasso&quot;</code> to <code>method = &quot;ridge&quot;</code></p>
<pre class="sourceCode r" id="cb51"><code class="sourceCode r"><div class="sourceLine" id="cb51-1" data-line-number="1">fit_enet &lt;-<span class="st"> </span><span class="kw">train</span>(points <span class="op">~</span><span class="st"> </span>.<span class="op">*</span>., </div>
<div class="sourceLine" id="cb51-2" data-line-number="2">                   <span class="dt">data =</span> training,</div>
<div class="sourceLine" id="cb51-3" data-line-number="3">                   <span class="dt">trControl =</span> train_control,</div>
<div class="sourceLine" id="cb51-4" data-line-number="4">                   <span class="dt">method =</span> <span class="st">&quot;enet&quot;</span>,</div>
<div class="sourceLine" id="cb51-5" data-line-number="5">                   <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))</div></code></pre>
<p>And let’s evaluate the model in the same two ways.</p>
<p>Easier test: average fit across training runs</p>
<pre class="sourceCode r" id="cb52"><code class="sourceCode r"><div class="sourceLine" id="cb52-1" data-line-number="1">avg_r2_enet &lt;-<span class="st"> </span><span class="kw">mean</span>(fit_enet<span class="op">$</span>results<span class="op">$</span>Rsquared)</div>
<div class="sourceLine" id="cb52-2" data-line-number="2">avg_RMSE_enet &lt;-<span class="st"> </span><span class="kw">mean</span>(fit_enet<span class="op">$</span>results<span class="op">$</span>RMSE)</div>
<div class="sourceLine" id="cb52-3" data-line-number="3"></div>
<div class="sourceLine" id="cb52-4" data-line-number="4">avg_r2_enet</div></code></pre>
<pre><code>## [1] 0.2480728</code></pre>
<pre class="sourceCode r" id="cb54"><code class="sourceCode r"><div class="sourceLine" id="cb54-1" data-line-number="1">avg_RMSE_enet</div></code></pre>
<pre><code>## [1] 2.923794</code></pre>
<p>Okay, so an <span class="math inline">\(\bar{R^2}\)</span> of 0.25, meaning we are explaining 24.97% of the variance (on average) in wine ratings with sentiment and price (and the interaction).</p>
<p>Harder test: how does it do with the holdout sample?</p>
<pre class="sourceCode r" id="cb56"><code class="sourceCode r"><div class="sourceLine" id="cb56-1" data-line-number="1">pred_enet &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_enet, <span class="dt">newdata =</span> testing)</div></code></pre>
<pre><code>## Loading required package: elasticnet</code></pre>
<pre><code>## Loading required package: lars</code></pre>
<pre><code>## Loaded lars 1.2</code></pre>
<pre class="sourceCode r" id="cb60"><code class="sourceCode r"><div class="sourceLine" id="cb60-1" data-line-number="1"><span class="co"># Gets R^2 and RMSE for enet model</span></div>
<div class="sourceLine" id="cb60-2" data-line-number="2">fitstat_enet &lt;-<span class="st"> </span><span class="kw">postResample</span>(<span class="dt">pred =</span> pred_enet, </div>
<div class="sourceLine" id="cb60-3" data-line-number="3">                                  <span class="dt">obs =</span> testing<span class="op">$</span>points)</div>
<div class="sourceLine" id="cb60-4" data-line-number="4">fitstat_enet</div></code></pre>
<pre><code>##      RMSE  Rsquared 
## 2.8348519 0.2411339</code></pre>
<p>Okay, so an <span class="math inline">\(R^2\)</span> of 0.24, meaning we are explaining 24.11% of the variance in wine ratings with sentiment and price (and the interaction).</p>
</section>
</section>
</section>
</section>
<section id="closing-thoughts" class="level1">
<h1><span class="header-section-number">4</span> Closing thoughts</h1>
<p>I want to mention a few things in closing. The first is that you’ll notice the three methods we tried in our example produced nearly identical fits. One reason for this is that we supplied a very small number of predictors (just 2 + an interaction, so 3 parameters). When you have many more predictors, these methods may start to differ a bit more (especially if the predictors are correlated, as we went over in the difference between ridge and lasso).</p>
<p>Finally, we split the data once into a training and test set. This is generally OK, BUT, if you’re using the holdout sample to evaluate models (like we did here), you wouldn’t want to use it to CHOOSE a model. That is, if we actually wanted to decide which of the three methods we wanted to use, the most defensible case would be to split the data into three sets:</p>
<ol type="1">
<li>model training</li>
<li>model selection</li>
<li>test / model evaluation</li>
</ol>
<p>This would keep the sort of purity of our test (model evaluation) data, and provide a good defense against overfitting.</p>
</section>
<section id="references" class="level1">
<h1><span class="header-section-number">5</span> References:</h1>
<p>Yarkoni, T., &amp; Westfall, J. (2017). Choosing prediction over explanation in psychology: Lessons from machine learning. <em>Perspectives on Psychological Science</em>, 12(6), 1100-1122.</p>
<p>Kuhn, M., &amp; Johnson, K. (2013). Applied predictive modeling. New York, NY: Spring-Verlag.</p>

</section>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "true";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
