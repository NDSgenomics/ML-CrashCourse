<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>ML Crashcourse Index</title>
  <meta name="description" content="ML Crashcourse Index">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="ML Crashcourse Index" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="<a href="https://github.com/uodatascience/ML-CrashCourse" class="uri">https://github.com/uodatascience/ML-CrashCourse</a>" />
  
  
  <meta name="github-repo" content="uodatascience/ML-CrashCourse" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="ML Crashcourse Index" />
  
  
  

<meta name="author" content="UO Data Science">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regularized-regression-in-caret.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Index</a></li>
<li class="chapter" data-level="2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html"><i class="fa fa-check"></i><b>2</b> What is Machine Learning?</a><ul>
<li class="chapter" data-level="2.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#a-broad-definition"><i class="fa fa-check"></i><b>2.1</b> A Broad Definition…</a></li>
<li class="chapter" data-level="2.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#major-classes-of-ml-problems"><i class="fa fa-check"></i><b>2.2</b> Major Classes of ML Problems</a></li>
<li class="chapter" data-level="2.3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#a-brief-history"><i class="fa fa-check"></i><b>2.3</b> A Brief History…</a></li>
<li class="chapter" data-level="2.4" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#common-ml-algorithms"><i class="fa fa-check"></i><b>2.4</b> Common ML Algorithms</a><ul>
<li class="chapter" data-level="2.4.1" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#decision-trees"><i class="fa fa-check"></i><b>2.4.1</b> <strong>Decision Trees</strong></a></li>
<li class="chapter" data-level="2.4.2" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#support-vector-machines"><i class="fa fa-check"></i><b>2.4.2</b> <strong>Support Vector Machines</strong></a></li>
<li class="chapter" data-level="2.4.3" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#artificial-neural-networks"><i class="fa fa-check"></i><b>2.4.3</b> <strong>Artificial Neural Networks</strong></a></li>
<li class="chapter" data-level="2.4.4" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#k-means-clustering"><i class="fa fa-check"></i><b>2.4.4</b> <strong>k-Means Clustering</strong></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="what-is-machine-learning.html"><a href="what-is-machine-learning.html#resources"><i class="fa fa-check"></i><b>2.5</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html"><i class="fa fa-check"></i><b>3</b> Regularized Regression in Caret</a><ul>
<li class="chapter" data-level="3.1" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#the-bias-variance-tradeoff-overfitting-and-cross-validation"><i class="fa fa-check"></i><b>3.1</b> The Bias-Variance Tradeoff, Overfitting, and Cross-Validation</a><ul>
<li class="chapter" data-level="3.1.1" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>3.1.1</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="3.1.2" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#overfitting"><i class="fa fa-check"></i><b>3.1.2</b> Overfitting</a></li>
<li class="chapter" data-level="3.1.3" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#cross-validation"><i class="fa fa-check"></i><b>3.1.3</b> Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#regularization"><i class="fa fa-check"></i><b>3.2</b> Regularization</a><ul>
<li class="chapter" data-level="3.2.1" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#ridge-all-of-these-features-matter-but-only-a-little-bit."><i class="fa fa-check"></i><b>3.2.1</b> Ridge: all of these features matter, but only a little bit.</a></li>
<li class="chapter" data-level="3.2.2" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#lasso-only-some-features-matter-and-they-might-matter-a-lot"><i class="fa fa-check"></i><b>3.2.2</b> Lasso: only some features matter, and they might matter a lot</a></li>
<li class="chapter" data-level="3.2.3" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#elastic-net-maybe-everything-matters-and-maybe-only-a-little-bit."><i class="fa fa-check"></i><b>3.2.3</b> Elastic Net: maybe everything matters, and maybe only a little bit.</a></li>
<li class="chapter" data-level="3.2.4" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#example-using-caret"><i class="fa fa-check"></i><b>3.2.4</b> Example using Caret</a></li>
<li class="chapter" data-level="3.2.5" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#extracting-features-a-brief-detour-into-text-analysis"><i class="fa fa-check"></i><b>3.2.5</b> Extracting features: a brief detour into text analysis</a></li>
<li class="chapter" data-level="3.2.6" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#modeling-with-caret"><i class="fa fa-check"></i><b>3.2.6</b> Modeling with caret</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#closing-thoughts"><i class="fa fa-check"></i><b>3.3</b> Closing thoughts</a></li>
<li class="chapter" data-level="3.4" data-path="regularized-regression-in-caret.html"><a href="regularized-regression-in-caret.html#references"><i class="fa fa-check"></i><b>3.4</b> References:</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification</a><ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#example-using-svm-classification-in-caret"><i class="fa fa-check"></i><b>4.1</b> Example using SVM classification in caret</a><ul>
<li class="chapter" data-level="4.1.1" data-path="classification.html"><a href="classification.html#example-using-lasso-logisic-regression-in-glmnet"><i class="fa fa-check"></i><b>4.1.1</b> Example using LASSO logisic regression in glmnet</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#other-stuff-that-should-be-in-this-document"><i class="fa fa-check"></i><b>4.2</b> Other stuff that should be in this document</a><ul>
<li class="chapter" data-level="4.2.1" data-path="classification.html"><a href="classification.html#classifiers"><i class="fa fa-check"></i><b>4.2.1</b> Classifiers</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification.html"><a href="classification.html#concepts"><i class="fa fa-check"></i><b>4.2.2</b> Concepts</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ML Crashcourse Index</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification" class="section level1">
<h1><span class="header-section-number">4</span> Classification</h1>
<p>Last week we focused on using machine learning to predict a continuous outcome. This week we’ll concentrate on predicting, or classifying, a dichotomous variable. We’ll compare two different classification algorithms: * Support vector machine (SVM) classifier * LASSO logistic regression</p>
<div id="example-using-svm-classification-in-caret" class="section level2">
<h2><span class="header-section-number">4.1</span> Example using SVM classification in caret</h2>
<p>Today we’ll use a <a href="https://www.kaggle.com/jboysen/us-perm-visas">dataset from Kaggle</a> with lots information about US permanent visas applications. The goal is to predict the outcome of the application from a bunch of variables.</p>
<div id="getting-set-up" class="section level4">
<h4><span class="header-section-number">4.1.0.1</span> Getting set up</h4>
<p>First, let’s load some packages.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load up our packages</span>
installif &lt;-<span class="st"> </span>function(packages){
  <span class="co"># give me a character vector of packages</span>
  for(p in packages){
    <span class="co"># and I will check if we have them</span>
    if(!<span class="kw">require</span>(p, <span class="dt">character.only =</span> <span class="ot">TRUE</span>)){
      <span class="co"># install them if we don&#39;t</span>
      <span class="kw">install.packages</span>(p)
    }
    <span class="co"># and load them if all goes well.</span>
    <span class="kw">require</span>(p, <span class="dt">character.only =</span> <span class="ot">TRUE</span>)
  }
}

package_list &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;tidyverse&quot;</span>, <span class="st">&quot;caret&quot;</span>, <span class="st">&quot;ROCR&quot;</span>, <span class="st">&quot;pROC&quot;</span>, <span class="st">&quot;knitr&quot;</span>, <span class="st">&quot;glmnet&quot;</span>, <span class="st">&quot;e1071&quot;</span>)

<span class="kw">installif</span>(package_list)</code></pre></div>
<pre><code>## Loading required package: tidyverse</code></pre>
<pre><code>## Warning: package &#39;tidyverse&#39; was built under R version 3.4.2</code></pre>
<pre><code>## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ ggplot2 2.2.1     ✔ purrr   0.2.4
## ✔ tibble  1.4.2     ✔ dplyr   0.7.4
## ✔ tidyr   0.7.2     ✔ stringr 1.2.0
## ✔ readr   1.1.1     ✔ forcats 0.2.0</code></pre>
<pre><code>## Warning: package &#39;tibble&#39; was built under R version 3.4.3</code></pre>
<pre><code>## Warning: package &#39;tidyr&#39; was built under R version 3.4.2</code></pre>
<pre><code>## Warning: package &#39;purrr&#39; was built under R version 3.4.2</code></pre>
<pre><code>## Warning: package &#39;dplyr&#39; was built under R version 3.4.2</code></pre>
<pre><code>## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre><code>## Loading required package: caret</code></pre>
<pre><code>## Warning: package &#39;caret&#39; was built under R version 3.4.3</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## 
## Attaching package: &#39;caret&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     lift</code></pre>
<pre><code>## Loading required package: ROCR</code></pre>
<pre><code>## Loading required package: gplots</code></pre>
<pre><code>## 
## Attaching package: &#39;gplots&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     lowess</code></pre>
<pre><code>## Loading required package: methods</code></pre>
<pre><code>## Loading required package: pROC</code></pre>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<pre><code>## Loading required package: knitr</code></pre>
<pre><code>## Loading required package: glmnet</code></pre>
<pre><code>## Warning: package &#39;glmnet&#39; was built under R version 3.4.2</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## 
## Attaching package: &#39;Matrix&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     expand</code></pre>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## 
## Attaching package: &#39;foreach&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:purrr&#39;:
## 
##     accumulate, when</code></pre>
<pre><code>## Loaded glmnet 2.0-13</code></pre>
<pre><code>## 
## Attaching package: &#39;glmnet&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:pROC&#39;:
## 
##     auc</code></pre>
<pre><code>## Loading required package: e1071</code></pre>
<p>Next, let’s load the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># clear the environment, just to be safe</span>
<span class="kw">rm</span>(<span class="dt">list=</span><span class="kw">ls</span>())

<span class="co"># load data</span>
data =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;files/us_perm_visas_10K.csv&#39;</span>, <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p>Time to tidy the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># remove columns with no useful data (i.e. all observations are NA or has no text)</span>
<span class="co">#data.reduced = data[,(colSums(is.na(data)) &lt; nrow(data)) &amp; (colSums(data == &#39;&#39;) &lt; nrow(data) | is.na(colSums(data == &#39;&#39;)))]</span>
<span class="co"># do we have to keep the broken original df? -jls</span>
data &lt;-<span class="st"> </span>data[,(<span class="kw">colSums</span>(<span class="kw">is.na</span>(data)) &lt;<span class="st"> </span><span class="kw">nrow</span>(data)) &amp;<span class="st"> </span>(<span class="kw">colSums</span>(data ==<span class="st"> &#39;&#39;</span>) &lt;<span class="st"> </span><span class="kw">nrow</span>(data) |<span class="st"> </span><span class="kw">is.na</span>(<span class="kw">colSums</span>(data ==<span class="st"> &#39;&#39;</span>)))]

<span class="co"># change text to upper case, select unique rows, recode dollar amounts, and move outcome variable to the first column</span>
data =<span class="st"> </span>data %&gt;%
<span class="st">  </span><span class="kw">as_tibble</span>() %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pw_amount_9089 =</span> <span class="kw">gsub</span>(<span class="st">&quot;,&quot;</span>, <span class="st">&quot;&quot;</span>, pw_amount_9089)) %&gt;%
<span class="st">  </span><span class="kw">extract</span>(pw_amount_9089, <span class="st">&quot;pw_amount_9089&quot;</span>, <span class="dt">regex =</span> <span class="st">&quot;([0-9]{1,}).00&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pw_amount_9089 =</span> <span class="kw">as.integer</span>(pw_amount_9089)) %&gt;%
<span class="st">  </span><span class="kw">mutate_if</span>(is.character, <span class="kw">funs</span>(toupper)) %&gt;%
<span class="st">  </span><span class="kw">mutate_if</span>(is.character, as.factor) %&gt;%
<span class="st">  </span><span class="kw">unique</span>(.) %&gt;%
<span class="st">  </span><span class="kw">select</span>(case_status, <span class="kw">everything</span>())

<span class="co"># sry am bad at pipe syntax -jls</span>
data$case_received_date &lt;-<span class="st"> </span><span class="kw">as.Date</span>(data$case_received_date)
data$decision_date &lt;-<span class="st"> </span><span class="kw">as.Date</span>(data$decision_date)</code></pre></div>
<p>For simplicity, let’s limit ourselves to cases that are either certified or denied.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># check levels</span>
<span class="kw">levels</span>(data$case_status)</code></pre></div>
<pre><code>## [1] &quot;CERTIFIED&quot; &quot;DENIED&quot;    &quot;WITHDRAWN&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># remove withdrawn cases and relevel</span>
data.relevel =<span class="st"> </span>data %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!case_status %in%<span class="st"> &quot;WITHDRAWN&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">case_status =</span> <span class="kw">as.factor</span>(<span class="kw">as.character</span>(case_status)))

<span class="co"># double check that it worked</span>
<span class="kw">levels</span>(data.relevel$case_status)</code></pre></div>
<pre><code>## [1] &quot;CERTIFIED&quot; &quot;DENIED&quot;</code></pre>
<p>Let’s take a look at the predictor variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(data.relevel)</code></pre></div>
<pre><code>## # A tibble: 6 x 125
##   case_status agent_city  agent_firm_name          agent_state case_number
##   &lt;fct&gt;       &lt;fct&gt;       &lt;fct&gt;                    &lt;fct&gt;       &lt;fct&gt;      
## 1 CERTIFIED   PHILADELPH… GREEN AND SPIEGEL LLC    PA          A-16222-40…
## 2 CERTIFIED   NEW YORK    LAW OFFICES OF BARRY SI… NY          A-16140-11…
## 3 CERTIFIED   ENOLA       LAW OFFICES OF KENDRA S… PA          A-16305-66…
## 4 CERTIFIED   &quot;&quot;          &quot;&quot;                       &quot;&quot;          A-16305-66…
## 5 CERTIFIED   &quot;&quot;          &quot;&quot;                       &quot;&quot;          A-16305-66…
## 6 CERTIFIED   ROCKVILLE   NIELSEN &amp; DOODY, LLC     MD          A-16302-66…
## # ... with 120 more variables: case_received_date &lt;date&gt;,
## #   class_of_admission &lt;fct&gt;, country_of_citizenship &lt;fct&gt;,
## #   decision_date &lt;date&gt;, employer_address_1 &lt;fct&gt;,
## #   employer_address_2 &lt;fct&gt;, employer_city &lt;fct&gt;, employer_country &lt;fct&gt;,
## #   employer_decl_info_title &lt;fct&gt;, employer_name &lt;fct&gt;,
## #   employer_num_employees &lt;dbl&gt;, employer_phone &lt;fct&gt;,
## #   employer_phone_ext &lt;int&gt;, employer_postal_code &lt;fct&gt;,
## #   employer_state &lt;fct&gt;, employer_yr_estab &lt;int&gt;,
## #   foreign_worker_info_city &lt;fct&gt;, foreign_worker_info_education &lt;fct&gt;,
## #   foreign_worker_info_inst &lt;fct&gt;, foreign_worker_info_major &lt;fct&gt;,
## #   foreign_worker_info_state &lt;fct&gt;, fw_info_alt_edu_experience &lt;fct&gt;,
## #   fw_info_birth_country &lt;fct&gt;, fw_info_education_other &lt;fct&gt;,
## #   fw_info_postal_code &lt;fct&gt;, fw_info_rel_occup_exp &lt;fct&gt;,
## #   fw_info_req_experience &lt;fct&gt;, fw_info_training_comp &lt;fct&gt;,
## #   fw_info_yr_rel_edu_completed &lt;int&gt;, fw_ownership_interest &lt;fct&gt;,
## #   ji_fw_live_on_premises &lt;fct&gt;, ji_live_in_dom_svc_contract &lt;fct&gt;,
## #   ji_live_in_domestic_service &lt;fct&gt;, ji_offered_to_sec_j_fw &lt;fct&gt;,
## #   job_info_alt_cmb_ed_oth_yrs &lt;int&gt;, job_info_alt_combo_ed &lt;fct&gt;,
## #   job_info_alt_combo_ed_exp &lt;fct&gt;, job_info_alt_combo_ed_other &lt;fct&gt;,
## #   job_info_alt_field &lt;fct&gt;, job_info_alt_field_name &lt;fct&gt;,
## #   job_info_alt_occ &lt;fct&gt;, job_info_alt_occ_job_title &lt;fct&gt;,
## #   job_info_alt_occ_num_months &lt;int&gt;, job_info_combo_occupation &lt;fct&gt;,
## #   job_info_education &lt;fct&gt;, job_info_education_other &lt;fct&gt;,
## #   job_info_experience &lt;fct&gt;, job_info_experience_num_months &lt;int&gt;,
## #   job_info_foreign_ed &lt;fct&gt;, job_info_foreign_lang_req &lt;fct&gt;,
## #   job_info_job_req_normal &lt;fct&gt;, job_info_job_title &lt;fct&gt;,
## #   job_info_major &lt;fct&gt;, job_info_training &lt;fct&gt;,
## #   job_info_training_field &lt;fct&gt;, job_info_training_num_months &lt;int&gt;,
## #   job_info_work_city &lt;fct&gt;, job_info_work_postal_code &lt;fct&gt;,
## #   job_info_work_state &lt;fct&gt;, naics_us_code &lt;int&gt;, naics_us_title &lt;fct&gt;,
## #   orig_case_no &lt;fct&gt;, orig_file_date &lt;fct&gt;,
## #   preparer_info_emp_completed &lt;fct&gt;, preparer_info_title &lt;fct&gt;,
## #   pw_amount_9089 &lt;int&gt;, pw_determ_date &lt;fct&gt;, pw_expire_date &lt;fct&gt;,
## #   pw_job_title_908 &lt;fct&gt;, pw_level_9089 &lt;fct&gt;, pw_soc_code &lt;fct&gt;,
## #   pw_soc_title &lt;fct&gt;, pw_source_name_9089 &lt;fct&gt;,
## #   pw_source_name_other_9089 &lt;fct&gt;, pw_track_num &lt;fct&gt;,
## #   pw_unit_of_pay_9089 &lt;fct&gt;, recr_info_barg_rep_notified &lt;fct&gt;,
## #   recr_info_coll_teach_comp_proc &lt;fct&gt;,
## #   recr_info_coll_univ_teacher &lt;fct&gt;,
## #   recr_info_employer_rec_payment &lt;fct&gt;, recr_info_first_ad_start &lt;fct&gt;,
## #   recr_info_job_fair_from &lt;fct&gt;, recr_info_job_fair_to &lt;fct&gt;,
## #   recr_info_on_campus_recr_from &lt;fct&gt;,
## #   recr_info_on_campus_recr_to &lt;fct&gt;,
## #   recr_info_prof_org_advert_from &lt;fct&gt;,
## #   recr_info_prof_org_advert_to &lt;fct&gt;, recr_info_professional_occ &lt;fct&gt;,
## #   recr_info_radio_tv_ad_from &lt;fct&gt;, recr_info_radio_tv_ad_to &lt;fct&gt;,
## #   recr_info_second_ad_start &lt;fct&gt;, recr_info_sunday_newspaper &lt;fct&gt;,
## #   recr_info_swa_job_order_end &lt;fct&gt;,
## #   recr_info_swa_job_order_start &lt;fct&gt;, refile &lt;fct&gt;,
## #   ri_1st_ad_newspaper_name &lt;fct&gt;, ri_2nd_ad_newspaper_name &lt;fct&gt;,
## #   ri_2nd_ad_newspaper_or_journal &lt;fct&gt;, ri_campus_placement_from &lt;fct&gt;,
## #   ri_campus_placement_to &lt;fct&gt;, …</code></pre>
<p>For the sake of time, we’re going to reduce the data in a couple of ways. There are more sophisticated ways to reduce the dimensionality while retaining the information from predictors (e.g. using PCA), but I’m just simply going to select the first 10 predictors. (Choosing some complete/potentially useful variables)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data.chop &lt;-<span class="st"> </span>data.relevel[,<span class="kw">c</span>(<span class="st">&quot;case_status&quot;</span>,<span class="st">&quot;case_received_date&quot;</span>, <span class="st">&quot;decision_date&quot;</span>, <span class="st">&quot;job_info_training&quot;</span>, 
                     <span class="st">&quot;job_info_foreign_ed&quot;</span>, <span class="st">&quot;employer_num_employees&quot;</span>, <span class="st">&quot;job_info_foreign_lang_req&quot;</span>,
                     <span class="st">&quot;foreign_worker_info_education&quot;</span>, <span class="st">&quot;fw_info_rel_occup_exp&quot;</span>,
                     <span class="st">&quot;fw_info_req_experience&quot;</span>, <span class="st">&quot;job_info_experience&quot;</span>)]
data.chop$case_received_date &lt;-<span class="st"> </span><span class="kw">as.Date</span>(data.chop$case_received_date)
data.chop$decision_date &lt;-<span class="st"> </span><span class="kw">as.Date</span>(data.chop$decision_date)

data.chop &lt;-<span class="st"> </span>data.chop[<span class="kw">complete.cases</span>(data.chop),]</code></pre></div>
<p>Next, let’s reduce the number of observations we have while still making sure that we have enough observations in each outcome category.</p>
<p>Let’s check the base rate of each outcome.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">table</span>(data.chop$case_status)/<span class="kw">nrow</span>(data.chop),<span class="dv">3</span>)</code></pre></div>
<pre><code>## 
## CERTIFIED    DENIED 
##     0.979     0.021</code></pre>
<p>There are very few denials (lucky applicants!), but this will cause problems for us later. We’re going to oversample so that we have a 10% denial rate.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">6523</span>)

n_denied &lt;-<span class="st"> </span><span class="kw">nrow</span>(data.chop[data.chop$case_status ==<span class="st"> &quot;DENIED&quot;</span>,])

<span class="co"># sample separtely within each level</span>
cert =<span class="st"> </span>data.chop %&gt;%
<span class="st">  </span><span class="kw">filter</span>(case_status %in%<span class="st"> &quot;CERTIFIED&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">sample_n</span>(n_denied*<span class="dv">9</span>)

den =<span class="st"> </span>data.chop %&gt;%
<span class="st">  </span><span class="kw">filter</span>(case_status %in%<span class="st"> &quot;DENIED&quot;</span>)
  <span class="co">#sample_n(192) # gonna use all the minority samples we can</span>

<span class="co"># join samples</span>
data.ml =<span class="st"> </span><span class="kw">bind_rows</span>(cert,den)

<span class="co"># check proportions</span>
<span class="kw">round</span>(<span class="kw">table</span>(data.ml$case_status)/<span class="kw">nrow</span>(data.ml),<span class="dv">3</span>)</code></pre></div>
<pre><code>## 
## CERTIFIED    DENIED 
##       0.9       0.1</code></pre>
</div>
<div id="overview-of-steps" class="section level4">
<h4><span class="header-section-number">4.1.0.2</span> Overview of steps</h4>
<ol style="list-style-type: decimal">
<li>Split the data into training and test samples</li>
<li>Set training parameters (e.g. number of k-folds)</li>
<li>Run model</li>
<li>Inspect fit indices and accuracy</li>
<li>Adjust model (if necessary)</li>
<li>Apply model to test data to assess out of sample accuracy</li>
</ol>
</div>
<div id="splitting-the-data" class="section level4">
<h4><span class="header-section-number">4.1.0.3</span> Splitting the data</h4>
<p>We want to both develop a model and assess how well it can predict application status in a separate sample, so we’ll split our data into training and test datasets. Let’s use 75% of the data in the training sample and the remaining 25% in the test sample.</p>
<p>To do this, we’ll use the <code>createDataPartition()</code> function in caret, which samples randomly within level of our outcome variable. This way we have the same proportion of outcomes in the training and test samples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set a seed so we all get the same dataframes</span>
<span class="kw">set.seed</span>(<span class="dv">6523</span>)

<span class="co"># split the data based on the outcome case_status</span>
in.train =<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> data.ml$case_status,
                                 <span class="dt">p =</span> .<span class="dv">75</span>,
                                 <span class="dt">list =</span> <span class="ot">FALSE</span>)

<span class="co"># check that it&#39;s actually 75%</span>
<span class="kw">nrow</span>(in.train) /<span class="st"> </span><span class="kw">nrow</span>(data.ml)</code></pre></div>
<pre><code>## [1] 0.75</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># subset the training data</span>
training =<span class="st"> </span>data.ml[in.train,]

<span class="co"># subset the test data (i.e. not in `in.train`)</span>
test =<span class="st"> </span>data.ml[-in.train,]

<span class="co"># check proportions</span>
<span class="kw">round</span>(<span class="kw">table</span>(training$case_status)/<span class="kw">nrow</span>(training),<span class="dv">3</span>)</code></pre></div>
<pre><code>## 
## CERTIFIED    DENIED 
##       0.9       0.1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">table</span>(test$case_status)/<span class="kw">nrow</span>(test),<span class="dv">3</span>)</code></pre></div>
<pre><code>## 
## CERTIFIED    DENIED 
##       0.9       0.1</code></pre>
<p>Before we begin training the classifier, let’s setup our training parameters using <code>trainControl()</code>. For the sake of time, let’s use a 3-fold cross-validation. You may want to select more folds and/or repeat the k-fold cross-validation with several different samples. To do that you’d specify <code>method = &quot;repeatedcv&quot;</code> and <code>repeats = [n repeats]</code>. However, to save time, we’ll just do a single 3-fold cross-validation. We also want to output the classification probabilities so that we can use the “ROC” metric below and save the predictions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train.control =<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, 
                             <span class="dt">number =</span> <span class="dv">3</span>,
                             <span class="dt">classProbs =</span> <span class="ot">TRUE</span>,
                             <span class="dt">savePredictions =</span> <span class="ot">TRUE</span>)</code></pre></div>
</div>
<div id="train-the-model" class="section level4">
<h4><span class="header-section-number">4.1.0.4</span> Train the model</h4>
<p>Now, we’ll train a support vector machine (i.e. <code>method = &quot;svmLinear&quot;</code>) to predict our outcome <code>case_status</code> from all variables in the training dataset (i.e. <code>case_status ~ .</code>) using the training parameters we specified above (i.e. <code>train.control</code>). The rest of the inputs are as follows: * <code>na.action  = na.pass</code> allows NAs to pass through the model without crashing * <code>preProcess = c(&quot;center&quot;, &quot;scale&quot;)</code> centers and scales the predictors so that they’re on the same scale * <code>metric = &quot;Accuracy&quot;</code> means that we’ll use accuracy to select the optimal model</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.svc &lt;-<span class="st"> </span><span class="kw">train</span>(case_status ~<span class="st"> </span>., 
                 <span class="dt">data =</span> training,
                 <span class="dt">method =</span> <span class="st">&quot;svmLinear&quot;</span>,
                 <span class="dt">trControl =</span> train.control,
                 <span class="dt">na.action  =</span> na.pass,
                 <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
                 <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>)

<span class="co"># fit.svc = readRDS(&quot;files/fit.svc&quot;) # if you try to run the model and it&#39;s taking too long, you can load the data with this command</span>
<span class="co"># saveRDS(fit.svc, &quot;files/fit.svc&quot;) # code to save the model</span></code></pre></div>
</div>
<div id="assess-the-model" class="section level4">
<h4><span class="header-section-number">4.1.0.5</span> Assess the model</h4>
<p>First, let’s check the mean accuracy of the model across cross-validation folds.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.svc</code></pre></div>
<pre><code>## Support Vector Machines with Linear Kernel 
## 
## 1440 samples
##   10 predictor
##    2 classes: &#39;CERTIFIED&#39;, &#39;DENIED&#39; 
## 
## Pre-processing: centered (21), scaled (21) 
## Resampling: Cross-Validated (3 fold) 
## Summary of sample sizes: 960, 960, 960 
## Resampling results:
## 
##   Accuracy   Kappa   
##   0.9618056  0.744382
## 
## Tuning parameter &#39;C&#39; was held constant at a value of 1</code></pre>
<p>Let’s check out the classification accuracy and kappa values on each fold.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.svc$resample</code></pre></div>
<pre><code>##    Accuracy     Kappa Resample
## 1 0.9625000 0.7500000    Fold1
## 2 0.9604167 0.7331461    Fold2
## 3 0.9625000 0.7500000    Fold3</code></pre>
<p>Let’s unpack this a little further and look at our false positive and false negative rates using the confusion matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confusionMatrix</span>(fit.svc)</code></pre></div>
<pre><code>## Cross-Validated (3 fold) Confusion Matrix 
## 
## (entries are percentual average cell counts across resamples)
##  
##            Reference
## Prediction  CERTIFIED DENIED
##   CERTIFIED      90.0    3.8
##   DENIED          0.0    6.2
##                             
##  Accuracy (average) : 0.9618</code></pre>
<p>We can also visualize this by looking at the receiver operator curve.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">roc</span>(<span class="dt">predictor =</span> fit.svc$pred$CERTIFIED, <span class="dt">response =</span> fit.svc$pred$obs))</code></pre></div>
<p><img src="03-Classification_in_R_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="optimize-the-model" class="section level4">
<h4><span class="header-section-number">4.1.0.6</span> Optimize the model</h4>
<p>To try to improve our accuracy, we can adjust various parameters.</p>
<p>First, let’s change the selection metric from accuracy to ROC to try to balance sensitivity and specificity.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.svc.roc &lt;-<span class="st"> </span><span class="kw">train</span>(case_status ~<span class="st"> </span>., 
                 <span class="dt">data =</span> training,
                 <span class="dt">method =</span> <span class="st">&quot;svmLinear&quot;</span>,
                 <span class="dt">trControl =</span> train.control,
                 <span class="dt">na.action  =</span> na.pass,
                 <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
                 <span class="dt">metric =</span> <span class="st">&quot;Kappa&quot;</span>) <span class="co"># says ROC is not an available metric</span>

<span class="co"># fit.svc.roc = readRDS(&quot;files/fit.svc.roc&quot;) # if you try to run the model and it&#39;s taking too long, you can load the data with this command</span>
<span class="co"># saveRDS(fit.svc.roc, &quot;files/fit.svc.roc&quot;) # code to save the model</span></code></pre></div>
<p>Let’s compare accuracy and kappa values</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.table =<span class="st"> </span><span class="kw">bind_rows</span>(fit.svc$results, fit.svc.roc$results)
<span class="kw">row.names</span>(fit.table) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Accuracy&quot;</span>, <span class="st">&quot;ROC&quot;</span>)
<span class="kw">kable</span>(fit.table, <span class="dt">format =</span> <span class="st">&quot;pandoc&quot;</span>, <span class="dt">digits =</span> <span class="dv">3</span>)</code></pre></div>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">C</th>
<th align="right">Accuracy</th>
<th align="right">Kappa</th>
<th align="right">AccuracySD</th>
<th align="right">KappaSD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td align="right">1</td>
<td align="right">0.962</td>
<td align="right">0.744</td>
<td align="right">0.001</td>
<td align="right">0.010</td>
</tr>
<tr class="even">
<td>ROC</td>
<td align="right">1</td>
<td align="right">0.962</td>
<td align="right">0.744</td>
<td align="right">0.004</td>
<td align="right">0.035</td>
</tr>
</tbody>
</table>
<p>Now let’s plot the ROC for both models</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">roc1=<span class="kw">roc</span>(<span class="dt">predictor =</span> fit.svc$pred$CERTIFIED, <span class="dt">response =</span> fit.svc$pred$obs)
roc2=<span class="kw">roc</span>(<span class="dt">predictor =</span> fit.svc.roc$pred$CERTIFIED, <span class="dt">response =</span> fit.svc.roc$pred$obs)

<span class="kw">plot</span>(roc1, <span class="dt">col =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="dv">1</span>, <span class="dt">main =</span> <span class="st">&quot;ROC&quot;</span>)
<span class="kw">plot</span>(roc2, <span class="dt">col =</span> <span class="dv">4</span>, <span class="dt">lty =</span> <span class="dv">1</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>)
<span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;ACC&quot;</span>, <span class="st">&quot;ROC&quot;</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>), <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>)</code></pre></div>
<p><img src="03-Classification_in_R_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Using the first model we ran, let’s tune the cost function (C).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># This doesn&#39;t work for me, turning warnings off off -jls</span>

<span class="co"># specify different values to assign to the cost function</span>
grid =<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">C =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.25</span>, <span class="fl">0.75</span>, <span class="dv">1</span>, <span class="fl">1.5</span>, <span class="dv">2</span>,<span class="dv">5</span>))

fit.svc.tune =<span class="st"> </span><span class="kw">train</span>(case_status ~<span class="st"> </span>., 
                 <span class="dt">data =</span> training,
                 <span class="dt">method =</span> <span class="st">&quot;svmLinear&quot;</span>,
                 <span class="dt">trControl =</span> train.control,
                 <span class="dt">na.action  =</span> na.pass,
                 <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
                 <span class="dt">metric =</span> <span class="st">&quot;Accuracy&quot;</span>,
                 <span class="dt">tuneGrid =</span> grid,
                 <span class="dt">tuneLength =</span> <span class="dv">10</span>)

<span class="co"># fit.svc.tune = readRDS(&quot;files/fit.svc.tune&quot;) # if you try to run the model and it&#39;s taking too long, you can load the data with this command</span>
<span class="co"># saveRDS(fit.svc.tune, &quot;files/fit.svc.tune&quot;) # code to save the model</span></code></pre></div>
<p>Let’s check the model results</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.svc.tune</code></pre></div>
<pre><code>## Support Vector Machines with Linear Kernel 
## 
## 1440 samples
##   10 predictor
##    2 classes: &#39;CERTIFIED&#39;, &#39;DENIED&#39; 
## 
## Pre-processing: centered (21), scaled (21) 
## Resampling: Cross-Validated (3 fold) 
## Summary of sample sizes: 960, 960, 960 
## Resampling results across tuning parameters:
## 
##   C     Accuracy   Kappa    
##   0.00        NaN        NaN
##   0.01  0.9618056  0.7438880
##   0.05  0.9618056  0.7438880
##   0.25  0.9618056  0.7438880
##   0.75  0.9618056  0.7438880
##   1.00  0.9618056  0.7438880
##   1.50  0.9618056  0.7438880
##   2.00  0.9618056  0.7438880
##   5.00  0.9604167  0.7330184
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was C = 0.01.</code></pre>
<p>And plot the accuracy as a function of the cost parameter C</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit.svc.tune)</code></pre></div>
<p><img src="03-Classification_in_R_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Now let’s plot the ROC for all three models</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">roc1=<span class="kw">roc</span>(<span class="dt">predictor =</span> fit.svc$pred$CERTIFIED, <span class="dt">response =</span> fit.svc$pred$obs)
roc2=<span class="kw">roc</span>(<span class="dt">predictor =</span> fit.svc.roc$pred$CERTIFIED, <span class="dt">response =</span> fit.svc.roc$pred$obs)
roc3=<span class="kw">roc</span>(<span class="dt">predictor =</span> fit.svc.tune$pred$CERTIFIED, <span class="dt">response =</span> fit.svc.tune$pred$obs)

<span class="kw">plot</span>(roc1, <span class="dt">col =</span> <span class="dv">1</span>, <span class="dt">lty =</span> <span class="dv">1</span>, <span class="dt">main =</span> <span class="st">&quot;ROC&quot;</span>)
<span class="kw">plot</span>(roc2, <span class="dt">col =</span> <span class="dv">4</span>, <span class="dt">lty =</span> <span class="dv">1</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>)
<span class="kw">plot</span>(roc2, <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>)
<span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;ACC&quot;</span>, <span class="st">&quot;ROC&quot;</span>, <span class="st">&quot;TUNED&quot;</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">2</span>), <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>), <span class="dt">bty =</span> <span class="st">&quot;n&quot;</span>)</code></pre></div>
<p><img src="03-Classification_in_R_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>And compare accuracy</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.table =<span class="st"> </span><span class="kw">bind_rows</span>(fit.svc$results, fit.svc.roc$results, <span class="kw">filter</span>(fit.svc.tune$results, C ==<span class="st"> </span>.<span class="dv">05</span>))
<span class="kw">row.names</span>(fit.table) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Accuracy&quot;</span>, <span class="st">&quot;ROC&quot;</span>, <span class="st">&quot;Tuned&quot;</span>)
<span class="kw">kable</span>(fit.table, <span class="dt">format =</span> <span class="st">&quot;pandoc&quot;</span>, <span class="dt">digits =</span> <span class="dv">3</span>)</code></pre></div>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">C</th>
<th align="right">Accuracy</th>
<th align="right">Kappa</th>
<th align="right">AccuracySD</th>
<th align="right">KappaSD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td align="right">1.00</td>
<td align="right">0.962</td>
<td align="right">0.744</td>
<td align="right">0.001</td>
<td align="right">0.010</td>
</tr>
<tr class="even">
<td>ROC</td>
<td align="right">1.00</td>
<td align="right">0.962</td>
<td align="right">0.744</td>
<td align="right">0.004</td>
<td align="right">0.035</td>
</tr>
<tr class="odd">
<td>Tuned</td>
<td align="right">0.05</td>
<td align="right">0.962</td>
<td align="right">0.744</td>
<td align="right">0.004</td>
<td align="right">0.035</td>
</tr>
</tbody>
</table>
</div>
<div id="test-in-holdout-sample" class="section level4">
<h4><span class="header-section-number">4.1.0.7</span> Test in holdout sample</h4>
<p>Let’s apply the best fitting model to the test data and see how well it performs in a new sample</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get predicted values for the test data</span>
test.pred =<span class="st"> </span><span class="kw">predict</span>(fit.svc, <span class="dt">newdata =</span> test)</code></pre></div>
<p>Let’s apply the best fitting model to the test data and see how well it performs in a new sample</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># get predicted values for the test data</span>
test.pred =<span class="st"> </span><span class="kw">predict</span>(fit.svc, <span class="dt">newdata =</span> test)</code></pre></div>
<p>To assess the performance, let’s check out the confusion matrix</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confusionMatrix</span>(test.pred, test$case_status)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  CERTIFIED DENIED
##   CERTIFIED       432     21
##   DENIED            0     27
##                                           
##                Accuracy : 0.9562          
##                  95% CI : (0.9339, 0.9727)
##     No Information Rate : 0.9             
##     P-Value [Acc &gt; NIR] : 4.195e-06       
##                                           
##                   Kappa : 0.6983          
##  Mcnemar&#39;s Test P-Value : 1.275e-05       
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.5625          
##          Pos Pred Value : 0.9536          
##          Neg Pred Value : 1.0000          
##              Prevalence : 0.9000          
##          Detection Rate : 0.9000          
##    Detection Prevalence : 0.9437          
##       Balanced Accuracy : 0.7812          
##                                           
##        &#39;Positive&#39; Class : CERTIFIED       
## </code></pre>
<p>So how well is this model really performing? By looking at the No Information Rate and the associated p-value, we see that while our model has good verall accuracy, it actually isn’t significantly better than simply guessing based on the base rates of the classes.</p>
<p>More on how to interpret the metrics in the confusion matrix <a href="https://www.hranalytics101.com/how-to-assess-model-accuracy-the-basics/#confusion-matrix-and-the-no-information-rate">here</a>.</p>
</div>
<div id="example-using-lasso-logisic-regression-in-glmnet" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Example using LASSO logisic regression in glmnet</h3>
<p>I’m sure there’s a way to do this using caret, but I couldn’t find a straightforward answer, so I’m using the glmnet package. The basic concepts are the same, but the syntax is slightly different. We also need to do some additional tidying to get the data in the correct format for glmnet.</p>
<p>We also need to convert any factors to dummy coded variables in the training and test data (which caret does internally). We’ll do that using <code>dummyVars()</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># dummy code training data</span>
dummy =<span class="st"> </span><span class="kw">dummyVars</span>(<span class="st">&quot; ~ .&quot;</span>, <span class="dt">data =</span> data.ml[,-<span class="dv">1</span>], <span class="dt">fullRank =</span> <span class="ot">TRUE</span>)
training.dummy =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">predict</span>(dummy, <span class="dt">newdata =</span> training))
training.dummy$case_status =<span class="st"> </span>training$case_status

training.dummy =<span class="st"> </span>training.dummy %&gt;%
<span class="st">  </span><span class="kw">select</span>(case_status, <span class="kw">everything</span>())

<span class="co"># dummy code testing data</span>
test.dummy =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">predict</span>(dummy, <span class="dt">newdata =</span> test))
test.dummy$case_status =<span class="st"> </span>test$case_status

test.dummy =<span class="st"> </span>test.dummy %&gt;%
<span class="st">  </span><span class="kw">select</span>(case_status, <span class="kw">everything</span>())

<span class="co"># print names</span>
<span class="kw">names</span>(training.dummy)</code></pre></div>
<pre><code>##  [1] &quot;case_status&quot;                              
##  [2] &quot;case_received_date&quot;                       
##  [3] &quot;decision_date&quot;                            
##  [4] &quot;job_info_training.N&quot;                      
##  [5] &quot;job_info_training.Y&quot;                      
##  [6] &quot;job_info_foreign_ed.N&quot;                    
##  [7] &quot;job_info_foreign_ed.Y&quot;                    
##  [8] &quot;employer_num_employees&quot;                   
##  [9] &quot;job_info_foreign_lang_req.N&quot;              
## [10] &quot;job_info_foreign_lang_req.Y&quot;              
## [11] &quot;foreign_worker_info_education.BACHELOR.S&quot; 
## [12] &quot;foreign_worker_info_education.DOCTORATE&quot;  
## [13] &quot;foreign_worker_info_education.HIGH.SCHOOL&quot;
## [14] &quot;foreign_worker_info_education.MASTER.S&quot;   
## [15] &quot;foreign_worker_info_education.NONE&quot;       
## [16] &quot;foreign_worker_info_education.OTHER&quot;      
## [17] &quot;fw_info_rel_occup_exp.N&quot;                  
## [18] &quot;fw_info_rel_occup_exp.Y&quot;                  
## [19] &quot;fw_info_req_experience.N&quot;                 
## [20] &quot;fw_info_req_experience.Y&quot;                 
## [21] &quot;job_info_experience.N&quot;                    
## [22] &quot;job_info_experience.Y&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># subset predictors and criterion and save as matrices</span>
x_train =<span class="st"> </span><span class="kw">as.matrix</span>(training.dummy[,-<span class="dv">1</span>])
y_train =<span class="st"> </span><span class="kw">as.matrix</span>(training.dummy[, <span class="dv">1</span>])</code></pre></div>
<p>Run the logistic regression model with 3 cross-validation folds, an alpha of 1 (i.e. the LASSO penalty, 0 = ridge penalty), scaled (i.e. standardize) predictors, using area under the ROC curve as our metric. This will allow us to determine what lambda parameter to use.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.log =<span class="st"> </span><span class="kw">cv.glmnet</span>(x_train, y_train, 
                   <span class="dt">family=</span><span class="st">&#39;binomial&#39;</span>, 
                   <span class="dt">alpha=</span><span class="dv">1</span>, 
                   <span class="dt">standardize=</span><span class="ot">TRUE</span>, 
                   <span class="dt">type.measure=</span><span class="st">&#39;auc&#39;</span>,
                   <span class="dt">nfolds =</span> <span class="dv">3</span>)</code></pre></div>
<p>Plot lambda versus fit metric AUC and print best lambda parameters</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plots</span>
<span class="kw">plot</span>(fit.log)</code></pre></div>
<p><img src="03-Classification_in_R_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit.log$glmnet.fit, <span class="dt">xvar=</span><span class="st">&quot;lambda&quot;</span>, <span class="dt">label=</span><span class="ot">TRUE</span>)</code></pre></div>
<p><img src="03-Classification_in_R_files/figure-html/unnamed-chunk-30-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># print lambdas</span>
fit.log$lambda.min</code></pre></div>
<pre><code>## [1] 0.0001084776</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit.log$lambda.1se</code></pre></div>
<pre><code>## [1] 0.00281502</code></pre>
<p>Let’s check the coefficient matrix to see which variables were shrunk</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(fit.log, <span class="dt">s=</span>fit.log$lambda.min)</code></pre></div>
<pre><code>## 22 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                                                       1
## (Intercept)                                8.427949e+02
## case_received_date                         1.562069e-02
## decision_date                             -6.477894e-02
## job_info_training.N                       -1.389921e-01
## job_info_training.Y                        9.361990e-16
## job_info_foreign_ed.N                     -1.699733e-01
## job_info_foreign_ed.Y                     -9.114585e-01
## employer_num_employees                     8.731786e-07
## job_info_foreign_lang_req.N               -1.086135e+00
## job_info_foreign_lang_req.Y                1.010521e-14
## foreign_worker_info_education.BACHELOR.S   .           
## foreign_worker_info_education.DOCTORATE   -1.090210e+00
## foreign_worker_info_education.HIGH.SCHOOL  2.599336e-01
## foreign_worker_info_education.MASTER.S    -6.006248e-01
## foreign_worker_info_education.NONE        -2.193841e-01
## foreign_worker_info_education.OTHER       -3.531508e-01
## fw_info_rel_occup_exp.N                    2.368421e-01
## fw_info_rel_occup_exp.Y                   -9.200490e-01
## fw_info_req_experience.N                   4.564199e-01
## fw_info_req_experience.Y                   7.322597e-01
## job_info_experience.N                      6.287093e-01
## job_info_experience.Y                     -1.319971e-14</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(fit.log, <span class="dt">s=</span>fit.log$lambda.1se)</code></pre></div>
<pre><code>## 22 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                                                       1
## (Intercept)                                8.802170e+02
## case_received_date                         9.802976e-03
## decision_date                             -6.117149e-02
## job_info_training.N                        .           
## job_info_training.Y                        .           
## job_info_foreign_ed.N                      .           
## job_info_foreign_ed.Y                     -5.886632e-01
## employer_num_employees                     .           
## job_info_foreign_lang_req.N               -9.627654e-01
## job_info_foreign_lang_req.Y                2.568094e-15
## foreign_worker_info_education.BACHELOR.S   .           
## foreign_worker_info_education.DOCTORATE   -5.781095e-01
## foreign_worker_info_education.HIGH.SCHOOL  3.082391e-01
## foreign_worker_info_education.MASTER.S    -4.326256e-01
## foreign_worker_info_education.NONE         .           
## foreign_worker_info_education.OTHER        .           
## fw_info_rel_occup_exp.N                    6.793113e-02
## fw_info_rel_occup_exp.Y                   -8.067368e-01
## fw_info_req_experience.N                  -6.968803e-02
## fw_info_req_experience.Y                   1.089085e-01
## job_info_experience.N                      .           
## job_info_experience.Y                      .</code></pre>
<p>Now let’s use the best lambda generated from running that model and apply it to our training sample</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">predicted.log =<span class="st"> </span><span class="kw">predict</span>(fit.log, <span class="dt">newx =</span> x_train, <span class="dt">s=</span>fit.log$lambda.1se, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</code></pre></div>
<p>Let’s figure out what cut point to use to determine whether a trial should be classified as CERTIFIED or DENIED</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot cutoff v. accuracy</span>
predicted =<span class="st"> </span><span class="kw">prediction</span>(predicted.log, y_train, <span class="dt">label.ordering =</span> <span class="ot">NULL</span>)
perf =<span class="st"> </span><span class="kw">performance</span>(predicted, <span class="dt">measure =</span> <span class="st">&quot;acc&quot;</span>)
perf.df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">cut=</span>perf@x.values[[<span class="dv">1</span>]],<span class="dt">acc=</span>perf@y.values[[<span class="dv">1</span>]])

<span class="kw">ggplot</span>(perf.df, <span class="kw">aes</span>(cut, acc)) +
<span class="st">  </span><span class="kw">geom_line</span>()</code></pre></div>
<p><img src="03-Classification_in_R_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot false v. true positive rate</span>
perf =<span class="st"> </span><span class="kw">performance</span>(predicted, <span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)
perf.df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">cut=</span>perf@alpha.values[[<span class="dv">1</span>]],<span class="dt">fpr=</span>perf@x.values[[<span class="dv">1</span>]],<span class="dt">tpr=</span>perf@y.values[[<span class="dv">1</span>]])

<span class="kw">ggplot</span>(perf.df, <span class="kw">aes</span>(fpr, tpr)) +
<span class="st">  </span><span class="kw">geom_line</span>()</code></pre></div>
<p><img src="03-Classification_in_R_files/figure-html/unnamed-chunk-33-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># plot specificity v. sensitivity</span>
perf =<span class="st"> </span><span class="kw">performance</span>(predicted, <span class="dt">measure =</span> <span class="st">&quot;sens&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;spec&quot;</span>)
perf.df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">cut=</span>perf@alpha.values[[<span class="dv">1</span>]],<span class="dt">sens=</span>perf@x.values[[<span class="dv">1</span>]],<span class="dt">spec=</span>perf@y.values[[<span class="dv">1</span>]])
<span class="kw">ggplot</span>(perf.df, <span class="kw">aes</span>(spec, sens)) +
<span class="st">  </span><span class="kw">geom_line</span>()</code></pre></div>
<p><img src="03-Classification_in_R_files/figure-html/unnamed-chunk-33-3.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(perf.df, <span class="kw">aes</span>(<span class="dt">x =</span> cut)) +
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> sens, <span class="dt">color =</span> <span class="st">&quot;sensitivity&quot;</span>)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> spec, <span class="dt">color =</span> <span class="st">&quot;specificity&quot;</span>))</code></pre></div>
<p><img src="03-Classification_in_R_files/figure-html/unnamed-chunk-33-4.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cut =<span class="st"> </span>perf@alpha.values[[<span class="dv">1</span>]][<span class="kw">which.max</span>(perf@x.values[[<span class="dv">1</span>]]+perf@y.values[[<span class="dv">1</span>]])]

<span class="co"># recode values based on cut</span>
predicted.cut =<span class="st"> </span><span class="kw">predict</span>(fit.log, <span class="dt">newx =</span> x_train, <span class="dt">s=</span>fit.log$lambda.1se, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)
predicted.cut[predicted.cut &gt;=<span class="st"> </span>cut] =<span class="st"> &quot;DENIED&quot;</span>
predicted.cut[predicted.cut &lt;<span class="st"> </span>cut] =<span class="st"> &quot;CERTIFIED&quot;</span></code></pre></div>
<p>Let’s take a look at the confusion matrix</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confusionMatrix</span>(predicted.cut, y_train)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  CERTIFIED DENIED
##   CERTIFIED      1296     55
##   DENIED            0     89
##                                           
##                Accuracy : 0.9618          
##                  95% CI : (0.9506, 0.9711)
##     No Information Rate : 0.9             
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.7444          
##  Mcnemar&#39;s Test P-Value : 3.305e-13       
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.6181          
##          Pos Pred Value : 0.9593          
##          Neg Pred Value : 1.0000          
##              Prevalence : 0.9000          
##          Detection Rate : 0.9000          
##    Detection Prevalence : 0.9382          
##       Balanced Accuracy : 0.8090          
##                                           
##        &#39;Positive&#39; Class : CERTIFIED       
## </code></pre>
</div>
</div>
<div id="other-stuff-that-should-be-in-this-document" class="section level2">
<h2><span class="header-section-number">4.2</span> Other stuff that should be in this document</h2>
<div id="classifiers" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Classifiers</h3>
<div id="support-vector-machine-classifier" class="section level4">
<h4><span class="header-section-number">4.2.1.1</span> Support vector machine classifier</h4>
</div>
<div id="lasso-logistic-regression" class="section level4">
<h4><span class="header-section-number">4.2.1.2</span> LASSO logistic regression</h4>
</div>
</div>
<div id="concepts" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Concepts</h3>
<div id="balancing-sensitivity-and-specificity" class="section level4">
<h4><span class="header-section-number">4.2.2.1</span> Balancing sensitivity and specificity</h4>
</div>
<div id="cut-points" class="section level4">
<h4><span class="header-section-number">4.2.2.2</span> Cut points</h4>
</div>
<div id="tuning" class="section level4">
<h4><span class="header-section-number">4.2.2.3</span> Tuning</h4>
</div>
<div id="reciever-operator-curves" class="section level4">
<h4><span class="header-section-number">4.2.2.4</span> Reciever operator curves</h4>
</div>
<div id="confusion-matrices" class="section level4">
<h4><span class="header-section-number">4.2.2.5</span> Confusion matrices</h4>
</div>
<div id="interpreting-weights" class="section level4">
<h4><span class="header-section-number">4.2.2.6</span> Interpreting weights</h4>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regularized-regression-in-caret.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
